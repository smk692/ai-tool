# Data Model: Hugging Face ì„ë² ë”© í†µí•©

**Feature**: 002-embedding-validation
**Version**: 1.0.0
**Last Updated**: 2025-01-17

---

## ê°œìš”

User Story 2 (Hugging Face ì„ë² ë”© í†µí•©)ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ëª¨ë¸ê³¼ ì—”í‹°í‹°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

---

## ì—”í‹°í‹° (Entities)

### 1. EmbeddingConfiguration

**ëª©ì **: ì„ë² ë”© ëª¨ë¸ ì„¤ì • ë° ê²€ì¦

**ìœ„ì¹˜**: `src/models/embedding.py` (âœ… ê¸°ì¡´ íŒŒì¼)

**ìƒíƒœ**: ì´ë¯¸ êµ¬í˜„ë¨ (Phase 1-2)

#### í•„ë“œ

```python
from dataclasses import dataclass
from enum import Enum
from typing import Optional

class DeviceType(Enum):
    """ì¶”ë¡  ë””ë°”ì´ìŠ¤ íƒ€ì…"""
    CPU = "cpu"
    CUDA = "cuda"
    MPS = "mps"  # Apple Silicon

@dataclass
class EmbeddingConfiguration:
    """ì„ë² ë”© ëª¨ë¸ ì„¤ì •"""

    model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"
    embedding_dim: int = 384
    device: DeviceType = DeviceType.CPU
    batch_size: int = 100
    max_sequence_length: int = 512
```

#### ê²€ì¦ ê·œì¹™

| í•„ë“œ | ê·œì¹™ | ì˜¤ë¥˜ ì²˜ë¦¬ |
|------|------|----------|
| `embedding_dim` | == 384 (ëª¨ë¸ ìŠ¤í™) | `ValueError` |
| `batch_size` | > 0 | `ValueError` |
| `max_sequence_length` | â‰¤ 512 (ëª¨ë¸ ì œí•œ) | Auto truncation |
| `device` | CPU, CUDA, MPS ì¤‘ í•˜ë‚˜ | `ValueError` |

#### ì˜ˆì‹œ

```python
# ê¸°ë³¸ ì„¤ì • (CPU)
config = EmbeddingConfiguration()

# GPU ì‚¬ìš©
config_gpu = EmbeddingConfiguration(
    device=DeviceType.CUDA,
    batch_size=200  # GPUì—ì„œ ë” í° ë°°ì¹˜ í¬ê¸°
)
```

---

### 2. HuggingFaceEmbedding

**ëª©ì **: Hugging Face ì„ë² ë”© ì„œë¹„ìŠ¤ (sentence-transformers)

**ìœ„ì¹˜**: `src/services/embeddings.py` (ğŸ†• T042ì—ì„œ ìƒì„±)

**ìƒíƒœ**: ë¯¸êµ¬í˜„ (Phase 4 - User Story 2)

#### ë©”ì„œë“œ

```python
from typing import List
from sentence_transformers import SentenceTransformer
from src.models.embedding import EmbeddingConfiguration

class HuggingFaceEmbedding:
    """
    Hugging Face sentence-transformers ê¸°ë°˜ ì„ë² ë”© ì„œë¹„ìŠ¤

    Responsibilities:
    - í…ìŠ¤íŠ¸ë¥¼ 384ì°¨ì› ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    - ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì²˜ë¦¬ëŸ‰ ìµœì í™”
    - L2 ì •ê·œí™”ëœ ë²¡í„° ìƒì„± (cosine similarityìš©)
    """

    def __init__(self, config: EmbeddingConfiguration):
        """
        ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”

        Args:
            config: ì„ë² ë”© ì„¤ì • (ëª¨ë¸ëª…, ë””ë°”ì´ìŠ¤ ë“±)

        Raises:
            RuntimeError: ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
        """
        self.config = config
        self.model: SentenceTransformer = None
        self.embedding_dim: int = config.embedding_dim

    def embed_text(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸ (ìµœëŒ€ 512 í† í°)

        Returns:
            384ì°¨ì› L2 ì •ê·œí™” ì„ë² ë”© ë²¡í„°

        Raises:
            ValueError: ë¹ˆ í…ìŠ¤íŠ¸ ì…ë ¥
        """

    def embed_texts(
        self,
        texts: List[str],
        batch_size: Optional[int] = None
    ) -> List[List[float]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”©

        Args:
            texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: config.batch_size)

        Returns:
            ê° í…ìŠ¤íŠ¸ì˜ 384ì°¨ì› ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸

        Raises:
            ValueError: ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì…ë ¥
        """

    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜ (384)"""

    def validate_model(self) -> bool:
        """
        ëª¨ë¸ ë¡œë”© ë° ê¸°ë³¸ ê¸°ëŠ¥ ê²€ì¦

        Returns:
            True if ëª¨ë¸ì´ ì •ìƒ ì‘ë™
        """
```

#### ìƒíƒœ ì „ì´

**N/A** - Stateless ì„œë¹„ìŠ¤ (ìƒíƒœ ì—†ìŒ)

#### ê²€ì¦ ê·œì¹™

| ì…ë ¥ | ì¡°ê±´ | ë™ì‘ |
|------|------|------|
| ë¹ˆ í…ìŠ¤íŠ¸ | `text.strip() == ""` | `ValueError` ë°œìƒ |
| ë¹ˆ ë¦¬ìŠ¤íŠ¸ | `len(texts) == 0` | `ValueError` ë°œìƒ |
| ê¸´ í…ìŠ¤íŠ¸ | `> 512 í† í°` | ìë™ truncation |
| ì°¨ì› ê²€ì¦ | `len(vector) != 384` | ë‚´ë¶€ ì˜¤ë¥˜ |

#### ì˜ˆì‹œ

```python
from src.services.embeddings import HuggingFaceEmbedding
from src.models.embedding import EmbeddingConfiguration

# ì´ˆê¸°í™”
config = EmbeddingConfiguration()
embedding_service = HuggingFaceEmbedding(config)

# ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
vector = embedding_service.embed_text("PostgreSQL íŠ¸ëœì­ì…˜")
assert len(vector) == 384

# ë°°ì¹˜ ì„ë² ë”©
texts = ["í•œêµ­ì–´ í…ìŠ¤íŠ¸", "English text", "æ··åˆ text"]
vectors = embedding_service.embed_texts(texts)
assert len(vectors) == 3
assert all(len(v) == 384 for v in vectors)
```

---

### 3. VectorStore (ì—…ë°ì´íŠ¸)

**ëª©ì **: ChromaDB ë²¡í„° ìŠ¤í† ì–´ (ì„ë² ë”© ì„œë¹„ìŠ¤ í†µí•©)

**ìœ„ì¹˜**: `src/services/vector_store.py` (ğŸ”„ T043ì—ì„œ ì—…ë°ì´íŠ¸)

**ìƒíƒœ**: ê¸°ì¡´ íŒŒì¼ ì—…ë°ì´íŠ¸ í•„ìš”

#### ë³€ê²½ ì‚¬í•­

**Before (Phase 1-2)**:
```python
class VectorStore:
    def __init__(self, config: ChromaDBConfig):
        self.config = config
        # ChromaDB ê¸°ë³¸ ì„ë² ë” ì‚¬ìš©
```

**After (Phase 4 - User Story 2)**:
```python
class VectorStore:
    def __init__(
        self,
        config: ChromaDBConfig,
        embedding_service: HuggingFaceEmbedding  # ğŸ†• ì¶”ê°€
    ):
        self.config = config
        self.embedding_service = embedding_service  # ğŸ†•
```

#### ì—…ë°ì´íŠ¸ ë©”ì„œë“œ

**`add_documents()` ë©”ì„œë“œ**:

```python
def add_documents(
    self,
    documents: List[str],
    metadatas: Optional[List[Dict]] = None,
    embeddings: Optional[List[List[float]]] = None,  # ê¸°ì¡´ íŒŒë¼ë¯¸í„°
    ids: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    ë¬¸ì„œë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€

    Args:
        documents: ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        metadatas: ê° ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°
        embeddings: Pre-computed embeddings (Noneì´ë©´ ìë™ ìƒì„±)
        ids: ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸

    Returns:
        {"success": bool, "count": int, "ids": List[str]}

    Note:
        embeddingsê°€ Noneì´ë©´ embedding_serviceë¡œ ìë™ ìƒì„±
    """
    # ğŸ†• ì„ë² ë”© ìë™ ìƒì„±
    if embeddings is None:
        embeddings = self.embedding_service.embed_texts(documents)

    # ChromaDBì— ì €ì¥ ...
```

**`query()` ë©”ì„œë“œ**:

```python
def query(
    self,
    query_text: str,
    top_k: int = 5,
    filter: Optional[Dict] = None
) -> Dict[str, Any]:
    """
    ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¡œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰

    Args:
        query_text: ê²€ìƒ‰ ì¿¼ë¦¬
        top_k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
        filter: ë©”íƒ€ë°ì´í„° í•„í„°

    Returns:
        {
            "documents": List[str],
            "metadatas": List[Dict],
            "distances": List[float],
            "ids": List[str]
        }
    """
    # ğŸ†• ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = self.embedding_service.embed_text(query_text)

    # ChromaDB ê²€ìƒ‰ ...
```

---

## ë°ì´í„° íë¦„ (Data Flow)

### 1. ë¬¸ì„œ ì¶”ê°€ í”„ë¡œì„¸ìŠ¤

```mermaid
sequenceDiagram
    participant User
    participant VectorStore
    participant EmbeddingService
    participant ChromaDB

    User->>VectorStore: add_documents(documents)
    VectorStore->>VectorStore: embeddings == None?
    VectorStore->>EmbeddingService: embed_texts(documents)
    EmbeddingService->>EmbeddingService: ë°°ì¹˜ ì²˜ë¦¬ (batch_size=100)
    EmbeddingService->>VectorStore: return 384-dim vectors
    VectorStore->>ChromaDB: collection.add(embeddings, documents, metadatas)
    ChromaDB->>VectorStore: success
    VectorStore->>User: {"success": True, "count": N}
```

**ë‹¨ê³„ë³„ ì„¤ëª…**:

1. **ì‚¬ìš©ì ìš”ì²­**: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì œê³µ
2. **ì„ë² ë”© ìƒì„±**: `HuggingFaceEmbedding.embed_texts()` í˜¸ì¶œ
   - ë°°ì¹˜ í¬ê¸° 100ìœ¼ë¡œ ì²˜ë¦¬
   - L2 ì •ê·œí™”ëœ 384ì°¨ì› ë²¡í„° ìƒì„±
   - ì§„í–‰ ìƒí™© í‘œì‹œ (tqdm progress bar)
3. **ì €ì¥**: ChromaDBì— ë²¡í„° + ë¬¸ì„œ + ë©”íƒ€ë°ì´í„° ì €ì¥
4. **ì‘ë‹µ**: ì„±ê³µ ì—¬ë¶€ ë° ì¶”ê°€ëœ ë¬¸ì„œ ìˆ˜ ë°˜í™˜

---

### 2. ì¿¼ë¦¬ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤

```mermaid
sequenceDiagram
    participant User
    participant VectorStore
    participant EmbeddingService
    participant ChromaDB

    User->>VectorStore: query(query_text, top_k=5)
    VectorStore->>EmbeddingService: embed_text(query_text)
    EmbeddingService->>EmbeddingService: ë‹¨ì¼ ì¿¼ë¦¬ ì²˜ë¦¬
    EmbeddingService->>VectorStore: return 384-dim vector
    VectorStore->>ChromaDB: collection.query(query_embedding, n_results=5)
    ChromaDB->>ChromaDB: Cosine similarity ê³„ì‚°
    ChromaDB->>VectorStore: Top-5 results
    VectorStore->>User: {"documents": [...], "metadatas": [...], "distances": [...]}
```

**ë‹¨ê³„ë³„ ì„¤ëª…**:

1. **ì‚¬ìš©ì ì¿¼ë¦¬**: ê²€ìƒ‰ í…ìŠ¤íŠ¸ ë° ê²°ê³¼ ìˆ˜(top_k) ì œê³µ
2. **ì¿¼ë¦¬ ì„ë² ë”©**: `HuggingFaceEmbedding.embed_text()` í˜¸ì¶œ
   - ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬
   - L2 ì •ê·œí™”ëœ 384ì°¨ì› ë²¡í„°
3. **ìœ ì‚¬ë„ ê²€ìƒ‰**: ChromaDBì—ì„œ Cosine similarity ê³„ì‚°
4. **ê²°ê³¼ ë°˜í™˜**: Top-K ë¬¸ì„œ, ë©”íƒ€ë°ì´í„°, ìœ ì‚¬ë„ ì ìˆ˜

---

## ê´€ê³„ë„ (Relationships)

```
EmbeddingConfiguration
    â””â”€â”€ ì‚¬ìš©ë¨ by
        â””â”€â”€ HuggingFaceEmbedding
            â””â”€â”€ ì‚¬ìš©ë¨ by
                â””â”€â”€ VectorStore
                    â””â”€â”€ ì‚¬ìš©ë¨ by
                        â”œâ”€â”€ KnowledgeChain (ì§€ì‹ ê²€ìƒ‰)
                        â””â”€â”€ MultiTurnChain (ëŒ€í™” ë©”ëª¨ë¦¬)
```

**ì„¤ëª…**:
- `EmbeddingConfiguration`: ì„¤ì • ì—”í‹°í‹° (ë¶ˆë³€)
- `HuggingFaceEmbedding`: ì„ë² ë”© ìƒì„± ì„œë¹„ìŠ¤
- `VectorStore`: ChromaDB ë˜í¼ (ì„ë² ë”© ì„œë¹„ìŠ¤ ì£¼ì…)
- `KnowledgeChain`, `MultiTurnChain`: ì²´ì¸ ë ˆë²¨ ì†Œë¹„ì

---

## í™˜ê²½ ë³€ìˆ˜ (Environment Variables)

```bash
# Embedding Model Configuration
EMBEDDING_MODEL_NAME=paraphrase-multilingual-MiniLM-L12-v2
EMBEDDING_DEVICE=cpu  # cpu | cuda | mps
EMBEDDING_BATCH_SIZE=100
EMBEDDING_MAX_SEQUENCE_LENGTH=512

# Vector Store Configuration
CHROMA_PERSIST_DIRECTORY=./data/chroma
CHROMA_COLLECTION_NAME=documents
CHROMA_DISTANCE_FUNCTION=cosine
```

**ì„¤ì • ìš°ì„ ìˆœìœ„**:
1. í™˜ê²½ ë³€ìˆ˜ (`.env`)
2. ê¸°ë³¸ê°’ (`config/settings.py`)

---

## ë°ì´í„° ìœ í˜• (Data Types)

### ì„ë² ë”© ë²¡í„°

**íƒ€ì…**: `List[float]`
**ì°¨ì›**: 384
**ì •ê·œí™”**: L2 normalized (ë²¡í„° í¬ê¸° = 1.0)
**ë²”ìœ„**: [-1.0, 1.0] (ê° ìš”ì†Œ)

**ì˜ˆì‹œ**:
```python
vector = [0.123, -0.456, 0.789, ...]  # 384ê°œ ìš”ì†Œ
magnitude = sum(x**2 for x in vector) ** 0.5
assert abs(magnitude - 1.0) < 1e-6  # L2 ì •ê·œí™” í™•ì¸
```

### ìœ ì‚¬ë„ ì ìˆ˜

**íƒ€ì…**: `float`
**ë²”ìœ„**: [0.0, 1.0] (Cosine similarity with L2 normalized vectors)
**í•´ì„**:
- 1.0: ì™„ì „ ë™ì¼
- 0.9-1.0: ë§¤ìš° ìœ ì‚¬
- 0.7-0.9: ìœ ì‚¬
- 0.5-0.7: ì–´ëŠ ì •ë„ ê´€ë ¨
- 0.0-0.5: ê´€ë ¨ ë‚®ìŒ

---

## ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| í•­ëª© | í¬ê¸° | ë¹„ê³  |
|------|------|------|
| ëª¨ë¸ | ~470MB | sentence-transformers ëª¨ë¸ |
| ë°°ì¹˜ 100ê°œ | ~150KB | 100 Ã— 384 Ã— 4 bytes |
| ChromaDB ì¸ë±ìŠ¤ | ê°€ë³€ | ë¬¸ì„œ ìˆ˜ì— ë¹„ë¡€ |

**ì´ ë©”ëª¨ë¦¬**: ~500MB (ëª¨ë¸) + ë¬¸ì„œ ìˆ˜ Ã— 1.5KB

### ì²˜ë¦¬ ì‹œê°„

| ì‘ì—… | CPU | GPU (CUDA) |
|------|-----|-----------|
| ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© | ~50ms | ~10ms |
| ë°°ì¹˜ 100ê°œ ì„ë² ë”© | ~2ì´ˆ | ~200ms |
| ì¿¼ë¦¬ ê²€ìƒ‰ (1000 docs) | <0.5ì´ˆ | <0.1ì´ˆ |

---

## í…ŒìŠ¤íŠ¸ ë°ì´í„°

### í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤

```python
korean_tests = [
    "ë°ì´í„°ë² ì´ìŠ¤ íŠ¸ëœì­ì…˜ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
    "PostgreSQL ì¸ë±ìŠ¤ ì¢…ë¥˜",
    "SQL ì¿¼ë¦¬ ìµœì í™” ë°©ë²•",
    "NoSQLê³¼ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ ì°¨ì´",
    "Pythonìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"
]
```

### ì˜ì–´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤

```python
english_tests = [
    "What is database transaction?",
    "Types of PostgreSQL indexes",
    "SQL query optimization techniques",
    "Difference between NoSQL and SQL",
    "How to connect database in Python"
]
```

### í˜¼í•© í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤

```python
mixed_tests = [
    "PostgreSQLì˜ ACID ì†ì„±",
    "MongoDB aggregate í•¨ìˆ˜ ì‚¬ìš©ë²•",
    "Python pandas ë°ì´í„° ë¶„ì„"
]
```

---

## ì°¸ê³  ìë£Œ

- [Hugging Face Model Card](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)
- [sentence-transformers Documentation](https://www.sbert.net/)
- [ChromaDB API Reference](https://docs.trychroma.com/reference/)

---

**Version**: 1.0.0
**Last Updated**: 2025-01-17
**Status**: Design Complete
**Next Step**: Implement `src/services/embeddings.py` (T042)
