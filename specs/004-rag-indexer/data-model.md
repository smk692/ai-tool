# ë°ì´í„° ëª¨ë¸: RAG Document Indexer

**ì‘ì„±ì¼**: 2025-12-05
**ë²„ì „**: 1.0

## ê°œìš”

RAG Document Indexerì˜ í•µì‹¬ ë°ì´í„° ì—”í‹°í‹°ì™€ ìƒíƒœ ê´€ë¦¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

---

## í•µì‹¬ ì—”í‹°í‹°

### 1. Source (ë°ì´í„° ì†ŒìŠ¤)

ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ì™¸ë¶€ ì†ŒìŠ¤ ì •ì˜ì…ë‹ˆë‹¤.

```python
from enum import Enum
from datetime import datetime
from pydantic import BaseModel, Field
from typing import Optional

class SourceType(str, Enum):
    NOTION = "notion"
    SWAGGER = "swagger"

class NotionSourceConfig(BaseModel):
    """Notion ì†ŒìŠ¤ ì„¤ì •"""
    page_ids: list[str] = Field(default_factory=list, description="ë™ê¸°í™”í•  í˜ì´ì§€ ID ëª©ë¡")
    database_ids: list[str] = Field(default_factory=list, description="ë™ê¸°í™”í•  ë°ì´í„°ë² ì´ìŠ¤ ID ëª©ë¡")
    include_children: bool = Field(default=True, description="í•˜ìœ„ í˜ì´ì§€ í¬í•¨ ì—¬ë¶€")

class SwaggerSourceConfig(BaseModel):
    """Swagger ì†ŒìŠ¤ ì„¤ì •"""
    url: str = Field(..., description="Swagger JSON URL")
    auth_header: Optional[str] = Field(None, description="ì¸ì¦ í—¤ë” (ì„ íƒ)")

class Source(BaseModel):
    """ë°ì´í„° ì†ŒìŠ¤ ì •ì˜"""
    id: str = Field(..., description="ì†ŒìŠ¤ ê³ ìœ  ì‹ë³„ì (UUID)")
    name: str = Field(..., description="ì†ŒìŠ¤ ì´ë¦„")
    source_type: SourceType = Field(..., description="ì†ŒìŠ¤ íƒ€ì…")
    config: NotionSourceConfig | SwaggerSourceConfig = Field(..., description="íƒ€ì…ë³„ ì„¤ì •")
    enabled: bool = Field(default=True, description="í™œì„±í™” ìƒíƒœ")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    last_synced_at: Optional[datetime] = Field(None, description="ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„")
```

---

### 2. Document (ë¬¸ì„œ)

ì†ŒìŠ¤ì—ì„œ ì¶”ì¶œí•œ ê°œë³„ ë¬¸ì„œ ë‹¨ìœ„ì…ë‹ˆë‹¤.

```python
class Document(BaseModel):
    """ì›ë³¸ ë¬¸ì„œ"""
    id: str = Field(..., description="ë¬¸ì„œ ê³ ìœ  ì‹ë³„ì (UUID)")
    source_id: str = Field(..., description="ì†Œì† ì†ŒìŠ¤ ID")
    external_id: str = Field(..., description="ì™¸ë¶€ ì‹œìŠ¤í…œ ID (Notion page_id ë“±)")
    title: str = Field(..., description="ë¬¸ì„œ ì œëª©")
    url: Optional[str] = Field(None, description="ì›ë³¸ URL")
    content_hash: str = Field(..., description="ì½˜í…ì¸  SHA256 í•´ì‹œ")
    metadata: dict = Field(default_factory=dict, description="ì¶”ê°€ ë©”íƒ€ë°ì´í„°")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    indexed_at: Optional[datetime] = Field(None, description="ì¸ë±ì‹± ì™„ë£Œ ì‹œê°„")
```

**ë©”íƒ€ë°ì´í„° ì˜ˆì‹œ**:
```python
# Notion í˜ì´ì§€
{
    "notion_type": "page",
    "parent_id": "abc123",
    "last_edited_by": "user@example.com",
    "icon": "ğŸ“„"
}

# Swagger ì—”ë“œí¬ì¸íŠ¸
{
    "swagger_version": "3.0.0",
    "method": "POST",
    "path": "/api/users",
    "tags": ["users"]
}
```

---

### 3. Chunk (ì²­í¬)

ë¬¸ì„œë¥¼ ë¶„í• í•œ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì¡°ê°ì…ë‹ˆë‹¤.

```python
class Chunk(BaseModel):
    """í…ìŠ¤íŠ¸ ì²­í¬"""
    id: str = Field(..., description="ì²­í¬ ê³ ìœ  ì‹ë³„ì (UUID)")
    document_id: str = Field(..., description="ì†Œì† ë¬¸ì„œ ID")
    chunk_index: int = Field(..., description="ë¬¸ì„œ ë‚´ ì²­í¬ ìˆœì„œ (0ë¶€í„° ì‹œì‘)")
    text: str = Field(..., description="ì²­í¬ í…ìŠ¤íŠ¸")
    token_count: int = Field(..., description="í† í° ìˆ˜ (ê·¼ì‚¬ì¹˜)")
    embedding: Optional[list[float]] = Field(None, description="ì„ë² ë”© ë²¡í„° (1024ì°¨ì›)")
    metadata: dict = Field(default_factory=dict, description="ì²­í¬ë³„ ë©”íƒ€ë°ì´í„°")
    created_at: datetime = Field(default_factory=datetime.utcnow)
```

**Qdrant í˜ì´ë¡œë“œ êµ¬ì¡°**:
```python
{
    "chunk_id": "uuid",
    "document_id": "uuid",
    "source_id": "uuid",
    "source_type": "notion",
    "title": "ë¬¸ì„œ ì œëª©",
    "url": "https://notion.so/...",
    "chunk_index": 0,
    "text": "ì²­í¬ í…ìŠ¤íŠ¸ ë‚´ìš©..."
}
```

---

### 4. SyncJob (ë™ê¸°í™” ì‘ì—…)

ë™ê¸°í™” ì‘ì—…ì˜ ì‹¤í–‰ ê¸°ë¡ì…ë‹ˆë‹¤.

```python
class SyncJobStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PARTIAL = "partial"  # ì¼ë¶€ ì„±ê³µ

class SyncJob(BaseModel):
    """ë™ê¸°í™” ì‘ì—…"""
    id: str = Field(..., description="ì‘ì—… ê³ ìœ  ì‹ë³„ì (UUID)")
    source_id: Optional[str] = Field(None, description="ëŒ€ìƒ ì†ŒìŠ¤ ID (Noneì´ë©´ ì „ì²´)")
    trigger: str = Field(..., description="íŠ¸ë¦¬ê±° íƒ€ì…: manual, scheduled")
    status: SyncJobStatus = Field(default=SyncJobStatus.PENDING)
    started_at: Optional[datetime] = Field(None)
    completed_at: Optional[datetime] = Field(None)

    # í†µê³„
    documents_processed: int = Field(default=0, description="ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜")
    documents_created: int = Field(default=0, description="ì‹ ê·œ ìƒì„± ë¬¸ì„œ ìˆ˜")
    documents_updated: int = Field(default=0, description="ì—…ë°ì´íŠ¸ ë¬¸ì„œ ìˆ˜")
    documents_deleted: int = Field(default=0, description="ì‚­ì œëœ ë¬¸ì„œ ìˆ˜")
    documents_skipped: int = Field(default=0, description="ìŠ¤í‚µëœ ë¬¸ì„œ ìˆ˜ (ë³€ê²½ ì—†ìŒ)")
    chunks_created: int = Field(default=0, description="ìƒì„±ëœ ì²­í¬ ìˆ˜")

    # ì—ëŸ¬ ì¶”ì 
    errors: list[dict] = Field(default_factory=list, description="ì—ëŸ¬ ëª©ë¡")
    error_message: Optional[str] = Field(None, description="ì£¼ìš” ì—ëŸ¬ ë©”ì‹œì§€")
```

**ì—ëŸ¬ êµ¬ì¡° ì˜ˆì‹œ**:
```python
{
    "document_id": "abc123",
    "error_type": "NotionAPIError",
    "message": "Rate limit exceeded",
    "timestamp": "2025-12-05T06:00:00Z",
    "retryable": True
}
```

---

## ìƒíƒœ ë‹¤ì´ì–´ê·¸ë¨

### SyncJob ìƒíƒœ ì „ì´

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ PENDING â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚ start()
                         â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”Œâ”€â”€â”€â”€â–ºâ”‚ RUNNING â”‚â—„â”€â”€â”€â”€â”
              â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚
              â”‚          â”‚          â”‚
              â”‚    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”    â”‚
              â”‚    â”‚           â”‚    â”‚
              â”‚    â–¼           â–¼    â”‚
         retryâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ partial success
              â”‚ â”‚FAILEDâ”‚  â”‚PARTIAL â”‚â”‚
              â”‚ â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
              â”‚                     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ all success
                         â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ COMPLETED â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Document ì¸ë±ì‹± í”Œë¡œìš°

```
 [Source]
    â”‚
    â”‚ fetch()
    â–¼
 [Raw Content]
    â”‚
    â”‚ parse()
    â–¼
 [Document]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                â”‚
    â”‚ hash match?    â”‚ yes
    â”‚                â–¼
    â”‚ no         [SKIP]
    â–¼
 [Chunking]
    â”‚
    â”‚ split()
    â–¼
 [Chunks]
    â”‚
    â”‚ embed()
    â–¼
 [Embeddings]
    â”‚
    â”‚ upsert()
    â–¼
 [Qdrant]
```

---

## ì €ì¥ì†Œ ì„¤ê³„

### ë¡œì»¬ ìƒíƒœ ì €ì¥ (JSON íŒŒì¼)

MVPì—ì„œëŠ” SQLite ëŒ€ì‹  JSON íŒŒì¼ë¡œ ìƒíƒœ ê´€ë¦¬í•©ë‹ˆë‹¤.

```
rag-indexer/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sources.json      # Source ëª©ë¡
â”‚   â”œâ”€â”€ documents.json    # Document ëª©ë¡ (í•´ì‹œ í¬í•¨)
â”‚   â””â”€â”€ sync_history.json # SyncJob ê¸°ë¡ (ìµœê·¼ 100ê°œ)
```

**sources.json êµ¬ì¡°**:
```json
{
  "sources": [
    {
      "id": "uuid-1",
      "name": "íŒ€ ìœ„í‚¤",
      "source_type": "notion",
      "config": {
        "page_ids": ["abc123"],
        "database_ids": [],
        "include_children": true
      },
      "enabled": true,
      "last_synced_at": "2025-12-05T06:00:00Z"
    }
  ]
}
```

### Qdrant ë²¡í„° ì €ì¥ì†Œ

**ì»¬ë ‰ì…˜ ì„¤ì •**:
```python
from qdrant_client.models import Distance, VectorParams

COLLECTION_CONFIG = {
    "collection_name": "rag_documents",
    "vectors_config": VectorParams(
        size=1024,  # intfloat/multilingual-e5-large-instruct
        distance=Distance.COSINE
    )
}
```

**ì¸ë±ìŠ¤ ì „ëµ**:
- Point ID: chunk_id (UUID)
- ë²¡í„°: 768ì°¨ì› float ë°°ì—´
- í˜ì´ë¡œë“œ: ê²€ìƒ‰/í•„í„°ë§ìš© ë©”íƒ€ë°ì´í„°

---

## ë°ì´í„° ë¬´ê²°ì„± ê·œì¹™

### ì œì•½ ì¡°ê±´

1. **Source ìœ ì¼ì„±**: `name`ì€ ì¤‘ë³µ ë¶ˆê°€
2. **Document ìœ ì¼ì„±**: ë™ì¼ ì†ŒìŠ¤ ë‚´ `external_id` ì¤‘ë³µ ë¶ˆê°€
3. **Chunk ìˆœì„œ**: ë™ì¼ ë¬¸ì„œ ë‚´ `chunk_index`ëŠ” 0ë¶€í„° ì—°ì†
4. **í•´ì‹œ ê²€ì¦**: `content_hash`ëŠ” SHA256 hex ë¬¸ìì—´ (64ì)

### ì‚­ì œ ì •ì±…

1. **Source ì‚­ì œ ì‹œ**: ì—°ê´€ëœ ëª¨ë“  Document, Chunk, Qdrant í¬ì¸íŠ¸ ì‚­ì œ
2. **Document ì‚­ì œ ì‹œ**: ì—°ê´€ëœ ëª¨ë“  Chunk, Qdrant í¬ì¸íŠ¸ ì‚­ì œ
3. **ì¬ë™ê¸°í™” ì‹œ**: ì›ë³¸ì— ì—†ëŠ” DocumentëŠ” soft delete í›„ ì •ë¦¬

### ë°ì´í„° ì •í•©ì„±

```python
class DataIntegrityChecker:
    """ë°ì´í„° ì •í•©ì„± ê²€ì¦"""

    async def verify_qdrant_sync(self) -> list[str]:
        """Qdrantì™€ ë¡œì»¬ ìƒíƒœ ì •í•©ì„± ê²€ì¦"""
        issues = []

        # 1. ë¡œì»¬ì— ìˆëŠ”ë° Qdrantì— ì—†ëŠ” ì²­í¬
        for chunk in local_chunks:
            if not await qdrant.point_exists(chunk.id):
                issues.append(f"Missing in Qdrant: {chunk.id}")

        # 2. Qdrantì— ìˆëŠ”ë° ë¡œì»¬ì— ì—†ëŠ” í¬ì¸íŠ¸
        orphan_points = await qdrant.find_orphans()
        for point_id in orphan_points:
            issues.append(f"Orphan in Qdrant: {point_id}")

        return issues
```

---

## ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

### ë²„ì „ ê´€ë¦¬

```python
DATA_VERSION = "1.0.0"

class DataMigration:
    """ë°ì´í„° ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜"""

    MIGRATIONS = {
        "1.0.0": None,  # ì´ˆê¸° ë²„ì „
        # "1.1.0": migrate_1_0_to_1_1,  # í–¥í›„ ë§ˆì´ê·¸ë ˆì´ì…˜
    }
```

### ë°±ì—… ì •ì±…

- ë™ê¸°í™” ì „ ìë™ ë°±ì—…: `data/backup/YYYYMMDD_HHMMSS/`
- ìµœê·¼ 7ì¼ ë°±ì—… ë³´ê´€
- ë³µêµ¬: `rag-indexer restore --backup <path>`

---

## ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­

### ë²Œí¬ ì‘ì—…

```python
# ì²­í¬ ì¼ê´„ upsert (100ê°œ ë‹¨ìœ„)
BATCH_SIZE = 100

async def bulk_upsert_chunks(chunks: list[Chunk]):
    for batch in chunked(chunks, BATCH_SIZE):
        points = [chunk_to_point(c) for c in batch]
        await qdrant.upsert(points=points)
```

### ë©”ëª¨ë¦¬ ê´€ë¦¬

- ëŒ€ìš©ëŸ‰ ë¬¸ì„œ: ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ì œí•œ
- ì„ë² ë”© ë°°ì¹˜: GPU ë©”ëª¨ë¦¬ ê³ ë ¤í•˜ì—¬ 32ê°œì”© ì²˜ë¦¬
- ìºì‹œ: Document í•´ì‹œ ë©”ëª¨ë¦¬ ìºì‹œ (LRU, ìµœëŒ€ 1000ê°œ)
