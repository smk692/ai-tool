# AI ë°ì´í„° ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ - êµ¬í˜„ ê³„íšì„œ

## ë¬¸ì„œ ì •ë³´

- **í”„ë¡œì íŠ¸ëª…**: AI ë°ì´í„° ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸
- **ë²„ì „**: 1.0.0
- **ì‘ì„±ì¼**: 2025ë…„ 1ì›”
- **ì „ì²´ ê¸°ê°„**: 4-6ì£¼
- **ì˜ˆìƒ ê³µìˆ˜**: 240-360 ì‹œê°„
- **ê¸°ìˆ  ìŠ¤íƒ**: 100% ì˜¤í”ˆì†ŒìŠ¤ (LLM API ì œì™¸)

---

## ğŸ“‹ ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ë¡œë“œë§µ](#1-í”„ë¡œì íŠ¸-ë¡œë“œë§µ)
2. [Phase 0: í™˜ê²½ ì„¤ì •](#2-phase-0-í™˜ê²½-ì„¤ì •-1ì£¼)
3. [Phase 1: RAG ì‹œìŠ¤í…œ](#3-phase-1-rag-ì‹œìŠ¤í…œ-1ì£¼)
4. [Phase 2: Text-to-SQL](#4-phase-2-text-to-sql-1ì£¼)
5. [Phase 3: ì¶”ê°€ ê¸°ëŠ¥](#5-phase-3-ì¶”ê°€-ê¸°ëŠ¥-1ì£¼)
6. [Phase 4: UI êµ¬ì¶•](#6-phase-4-ui-êµ¬ì¶•-1ì£¼)
7. [Phase 5: í…ŒìŠ¤íŒ…](#7-phase-5-í…ŒìŠ¤íŒ…-1ì£¼)
8. [Phase 6: ë°°í¬](#8-phase-6-ë°°í¬-ì„ íƒ)
9. [ë¦¬ìŠ¤í¬ ê´€ë¦¬](#9-ë¦¬ìŠ¤í¬-ê´€ë¦¬)
10. [í’ˆì§ˆ ê´€ë¦¬](#10-í’ˆì§ˆ-ê´€ë¦¬)

---

## 1. í”„ë¡œì íŠ¸ ë¡œë“œë§µ

### 1.1 ì „ì²´ ì¼ì •

```mermaid
gantt
    title AI ë°ì´í„° ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ êµ¬í˜„ ì¼ì •
    dateFormat  YYYY-MM-DD
    section ì¤€ë¹„
    í™˜ê²½ ì„¤ì •           :p0, 2025-01-15, 1w
    section í•µì‹¬ ê¸°ëŠ¥
    RAG ì‹œìŠ¤í…œ          :p1, after p0, 1w
    Text-to-SQL         :p2, after p1, 1w
    section í™•ì¥ ê¸°ëŠ¥
    ì¶”ê°€ ê¸°ëŠ¥ ê°œë°œ      :p3, after p2, 1w
    section UI/ë°°í¬
    Streamlit UI        :p4, after p3, 1w
    í…ŒìŠ¤íŒ… ë° ìµœì í™”    :p5, after p4, 1w
    ë°°í¬ (ì„ íƒ)         :p6, after p5, 3d
```

### 1.2 ë§ˆì¼ìŠ¤í†¤

| ë§ˆì¼ìŠ¤í†¤ | ì™„ë£Œ ê¸°ì¤€ | ê¸°í•œ |
|----------|-----------|------|
| **M1**: ê°œë°œ í™˜ê²½ êµ¬ì¶• | Docker, DB, API í‚¤ ì„¤ì • ì™„ë£Œ | Week 1 |
| **M2**: RAG ì‹œìŠ¤í…œ ì™„ì„± | Vector + BM25 ê²€ìƒ‰ ì‘ë™ | Week 2 |
| **M3**: MVP ì™„ì„± | Text-to-SQL ê¸°ë³¸ ê¸°ëŠ¥ ì‘ë™ | Week 3 |
| **M4**: ê¸°ëŠ¥ í™•ì¥ | Data Discovery, Memory ì™„ì„± | Week 4 |
| **M5**: UI ì™„ì„± | Streamlit ì¸í„°í˜ì´ìŠ¤ ì™„ì„± | Week 5 |
| **M6**: í”„ë¡œë•ì…˜ ë°°í¬ | Docker Compose ë°°í¬ ì™„ë£Œ | Week 6 |

### 1.3 íŒ€ êµ¬ì„± (ê¶Œì¥)

| ì—­í•  | ì¸ì› | ì±…ì„ |
|------|------|------|
| **Backend ê°œë°œì** | 1-2ëª… | RAG, Text-to-SQL, API |
| **Frontend ê°œë°œì** | 1ëª… | Streamlit UI |
| **ë°ì´í„° ì—”ì§€ë‹ˆì–´** | 1ëª… | ë©”íƒ€ë°ì´í„°, Few-shot |
| **QA ì—”ì§€ë‹ˆì–´** | 1ëª… | í…ŒìŠ¤íŒ…, í’ˆì§ˆ ë³´ì¦ |

> **ìµœì†Œ êµ¬ì„±**: 1ëª… (Full-stack)ìœ¼ë¡œë„ ê°€ëŠ¥ (6ì£¼ ì†Œìš”)

---

## 2. Phase 0: í™˜ê²½ ì„¤ì • (1ì£¼)

### 2.1 ëª©í‘œ

í”„ë¡œì íŠ¸ ê°œë°œì„ ìœ„í•œ ê¸°ë³¸ í™˜ê²½ êµ¬ì¶• ì™„ë£Œ

### 2.2 ì‘ì—… ëª©ë¡

#### 2.2.1 Python í™˜ê²½ ì„¤ì •

**ì†Œìš” ì‹œê°„**: 2ì‹œê°„

```bash
# 1. Python 3.10+ ì„¤ì¹˜ í™•ì¸
python --version  # 3.10 ì´ìƒ

# 2. ê°€ìƒ í™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ í¸ì§‘ (API í‚¤ ë“±)
```

#### 2.2.2 ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •

**ì†Œìš” ì‹œê°„**: 4ì‹œê°„

**PostgreSQL ì„¤ì¹˜**:
```bash
# Dockerë¡œ PostgreSQL ì‹¤í–‰
docker run --name postgres-db \
  -e POSTGRES_PASSWORD=your_password \
  -e POSTGRES_DB=your_database \
  -p 5432:5432 \
  -d postgres:15
```

**í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„**:
```sql
-- ìƒ˜í”Œ í…Œì´ë¸” ìƒì„±
CREATE TABLE users (
    id BIGSERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(20) DEFAULT 'active'
);

CREATE TABLE orders (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT REFERENCES users(id),
    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    total_amount DECIMAL(10, 2),
    status VARCHAR(20)
);

-- ìƒ˜í”Œ ë°ì´í„° ì‚½ì…
INSERT INTO users (email, created_at, status)
SELECT
    'user' || i || '@example.com',
    CURRENT_TIMESTAMP - (random() * INTERVAL '365 days'),
    CASE WHEN random() > 0.1 THEN 'active' ELSE 'inactive' END
FROM generate_series(1, 10000) i;

INSERT INTO orders (user_id, order_date, total_amount, status)
SELECT
    floor(random() * 10000 + 1)::BIGINT,
    CURRENT_TIMESTAMP - (random() * INTERVAL '180 days'),
    (random() * 500 + 10)::DECIMAL(10, 2),
    CASE
        WHEN random() < 0.7 THEN 'completed'
        WHEN random() < 0.9 THEN 'pending'
        ELSE 'cancelled'
    END
FROM generate_series(1, 50000) i;
```

#### 2.2.3 LLM API ì„¤ì •

**ì†Œìš” ì‹œê°„**: 1ì‹œê°„

1. **OpenAI API í‚¤ ë°œê¸‰**:
   - https://platform.openai.com/api-keys ì ‘ì†
   - API í‚¤ ìƒì„±
   - `.env` íŒŒì¼ì— ì €ì¥

2. **API í…ŒìŠ¤íŠ¸**:
```python
from langchain_openai import AzureChatOpenAI
from dotenv import load_dotenv
import os

load_dotenv()

llm = AzureChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    model_name="gpt-4o",
    temperature=0
)

# í…ŒìŠ¤íŠ¸
response = llm.invoke("Hello, world!")
print(response.content)
```

#### 2.2.4 í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±

**ì†Œìš” ì‹œê°„**: 1ì‹œê°„

```bash
mkdir -p ai-data-assistant/{src,data,tests,docs}
cd ai-data-assistant

# ë””ë ‰í† ë¦¬ êµ¬ì¡°
tree -L 2
# ai-data-assistant/
# â”œâ”€â”€ src/
# â”‚   â”œâ”€â”€ chains/          # LangChain ì²´ì¸
# â”‚   â”œâ”€â”€ rag/             # RAG ì‹œìŠ¤í…œ
# â”‚   â”œâ”€â”€ database/        # DB ì—°ê²°
# â”‚   â”œâ”€â”€ memory/          # ëŒ€í™” ë©”ëª¨ë¦¬
# â”‚   â””â”€â”€ ui/              # Streamlit UI
# â”œâ”€â”€ data/
# â”‚   â”œâ”€â”€ metadata/        # í…Œì´ë¸” ë©”íƒ€ë°ì´í„°
# â”‚   â”œâ”€â”€ fewshot/         # Few-shot ì˜ˆì œ
# â”‚   â””â”€â”€ documents/       # ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ
# â”œâ”€â”€ tests/
# â”‚   â”œâ”€â”€ test_rag.py
# â”‚   â”œâ”€â”€ test_text_to_sql.py
# â”‚   â””â”€â”€ test_chains.py
# â”œâ”€â”€ docs/
# â”œâ”€â”€ .env
# â”œâ”€â”€ requirements.txt
# â”œâ”€â”€ docker-compose.yml
# â””â”€â”€ README.md
```

### 2.3 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Python 3.10+ ì„¤ì¹˜
- [ ] ê°€ìƒ í™˜ê²½ ìƒì„± ë° í™œì„±í™”
- [ ] requirements.txt ì‘ì„± ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜
- [ ] PostgreSQL Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰
- [ ] ìƒ˜í”Œ ë°ì´í„° ìƒì„± (users, orders í…Œì´ë¸”)
- [ ] OpenAI API í‚¤ ë°œê¸‰ ë° ì„¤ì •
- [ ] API ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] Git ì €ì¥ì†Œ ì´ˆê¸°í™”
- [ ] README.md ì‘ì„±

### 2.4 ì‚°ì¶œë¬¼

- âœ… ê°œë°œ í™˜ê²½ ì™„ì„±
- âœ… PostgreSQL DB + ìƒ˜í”Œ ë°ì´í„°
- âœ… OpenAI API ì—°ë™
- âœ… í”„ë¡œì íŠ¸ êµ¬ì¡°

---

## 3. Phase 1: RAG ì‹œìŠ¤í…œ (1ì£¼)

### 3.1 ëª©í‘œ

Vector DB + BM25 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•

### 3.2 ì‘ì—… ëª©ë¡

#### 3.2.1 Vector Store êµ¬ì¶• (2ì¼)

**ì†Œìš” ì‹œê°„**: 16ì‹œê°„

**Day 1**: ChromaDB ì„¤ì • ë° í…ŒìŠ¤íŠ¸

```python
# src/rag/vector_store.py

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from typing import List, Dict

class VectorStore:
    def __init__(self, persist_directory="./chromadb_data"):
        self.client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=persist_directory,
            anonymized_telemetry=False
        ))

        self.embed_model = SentenceTransformer(
            'paraphrase-multilingual-MiniLM-L12-v2'
        )

        self.collection = self.client.get_or_create_collection(
            name="knowledge_base",
            metadata={"description": "í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ë° ë¬¸ì„œ"}
        )

    def add_documents(self, documents: List[Dict]):
        """ë¬¸ì„œ ì¶”ê°€"""
        embeddings = self.embed_model.encode(
            [doc["content"] for doc in documents],
            normalize_embeddings=True
        )

        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=[doc["content"] for doc in documents],
            metadatas=[doc.get("metadata", {}) for doc in documents],
            ids=[doc.get("id", str(i)) for i, doc in enumerate(documents)]
        )

    def search(self, query: str, top_k: int = 5):
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        query_embedding = self.embed_model.encode(
            query, normalize_embeddings=True
        )

        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k
        )

        return results
```

**Day 2**: ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ë° ì¸ë±ì‹±

```python
# src/database/metadata_collector.py

import psycopg2
from typing import Dict, List

class MetadataCollector:
    def __init__(self, db_connection):
        self.conn = db_connection

    def collect_all_metadata(self) -> List[Dict]:
        """ëª¨ë“  í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        tables = self.get_tables()
        metadata_docs = []

        for table in tables:
            doc = self.build_table_document(table)
            metadata_docs.append(doc)

        return metadata_docs

    def get_tables(self) -> List[str]:
        """í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = 'public'
        """)
        return [row[0] for row in cursor.fetchall()]

    def build_table_document(self, table_name: str) -> Dict:
        """í…Œì´ë¸” ë¬¸ì„œ ìƒì„±"""
        # ì¹¼ëŸ¼ ì •ë³´ ìˆ˜ì§‘
        columns = self.get_columns(table_name)

        # ìƒ˜í”Œ ê°’ ìˆ˜ì§‘
        sample_values = self.get_sample_values(table_name)

        # ë¬¸ì„œ êµ¬ì„±
        content = f"""
í…Œì´ë¸”: {table_name}
ì„¤ëª…: [ìˆ˜ë™ ì…ë ¥ í•„ìš”]
ì¹¼ëŸ¼: {', '.join([f"{c['name']} ({c['type']})" for c in columns])}
ìƒ˜í”Œ ê°’: {sample_values}
"""

        return {
            "id": f"table_{table_name}",
            "content": content,
            "metadata": {
                "type": "table",
                "table_name": table_name
            }
        }

    def get_columns(self, table_name: str) -> List[Dict]:
        cursor = self.conn.cursor()
        cursor.execute(f"""
            SELECT column_name, data_type
            FROM information_schema.columns
            WHERE table_name = '{table_name}'
        """)
        return [{"name": row[0], "type": row[1]} for row in cursor.fetchall()]

    def get_sample_values(self, table_name: str, limit=3) -> str:
        cursor = self.conn.cursor()
        cursor.execute(f"SELECT * FROM {table_name} LIMIT {limit}")
        rows = cursor.fetchall()
        return str(rows)

# ì‹¤í–‰
conn = psycopg2.connect("postgresql://user:pass@localhost/db")
collector = MetadataCollector(conn)
metadata = collector.collect_all_metadata()

# Vector DBì— ì €ì¥
vector_store = VectorStore()
vector_store.add_documents(metadata)
```

#### 3.2.2 BM25 ê²€ìƒ‰ êµ¬í˜„ (1ì¼)

**ì†Œìš” ì‹œê°„**: 8ì‹œê°„

```python
# src/rag/bm25_search.py

from rank_bm25 import BM25Okapi
import pickle
from typing import List, Dict

class BM25Search:
    def __init__(self, corpus_path="bm25_corpus.pkl"):
        self.corpus_path = corpus_path
        self.bm25 = None
        self.documents = []

    def build_index(self, documents: List[Dict]):
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        self.documents = documents
        tokenized_corpus = [self.tokenize(doc["content"]) for doc in documents]
        self.bm25 = BM25Okapi(tokenized_corpus)

        # ì €ì¥
        with open(self.corpus_path, "wb") as f:
            pickle.dump((self.bm25, self.documents), f)

    def load_index(self):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        with open(self.corpus_path, "rb") as f:
            self.bm25, self.documents = pickle.load(f)

    def tokenize(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì§•"""
        return text.lower().split()

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """ê²€ìƒ‰"""
        tokenized_query = self.tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)

        # ìƒìœ„ kê°œ
        top_indices = sorted(
            range(len(scores)),
            key=lambda i: scores[i],
            reverse=True
        )[:top_k]

        results = []
        for idx in top_indices:
            results.append({
                "document": self.documents[idx],
                "score": float(scores[idx])
            })

        return results
```

#### 3.2.3 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬í˜„ (2ì¼)

**ì†Œìš” ì‹œê°„**: 16ì‹œê°„

```python
# src/rag/hybrid_search.py

from typing import List, Dict
from .vector_store import VectorStore
from .bm25_search import BM25Search

class HybridSearch:
    def __init__(self):
        self.vector_store = VectorStore()
        self.bm25_search = BM25Search()

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (RRF)"""

        # 1. Vector ê²€ìƒ‰
        vector_results = self.vector_store.search(query, top_k=10)

        # 2. BM25 ê²€ìƒ‰
        bm25_results = self.bm25_search.search(query, top_k=10)

        # 3. Reciprocal Rank Fusion
        scores = {}

        # Vector ì ìˆ˜
        for i, doc_id in enumerate(vector_results['ids'][0]):
            scores[doc_id] = scores.get(doc_id, 0) + 1 / (i + 1 + 60)

        # BM25 ì ìˆ˜
        for i, result in enumerate(bm25_results):
            doc_id = result['document']['id']
            scores[doc_id] = scores.get(doc_id, 0) + 1 / (i + 1 + 60)

        # 4. ì •ë ¬
        sorted_ids = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]

        # 5. ê²°ê³¼ ë°˜í™˜
        final_results = []
        for doc_id, score in sorted_ids:
            # ë¬¸ì„œ ë‚´ìš© ì¡°íšŒ
            doc_content = self._get_document(doc_id, vector_results, bm25_results)
            final_results.append({
                "id": doc_id,
                "content": doc_content,
                "rrf_score": score
            })

        return final_results

    def _get_document(self, doc_id, vector_results, bm25_results):
        # Vector ê²°ê³¼ì—ì„œ ì°¾ê¸°
        if doc_id in vector_results['ids'][0]:
            idx = vector_results['ids'][0].index(doc_id)
            return vector_results['documents'][0][idx]

        # BM25 ê²°ê³¼ì—ì„œ ì°¾ê¸°
        for result in bm25_results:
            if result['document']['id'] == doc_id:
                return result['document']['content']

        return None
```

### 3.3 í…ŒìŠ¤íŠ¸

```python
# tests/test_rag.py

import unittest
from src.rag.hybrid_search import HybridSearch

class TestRAG(unittest.TestCase):
    def setUp(self):
        self.search = HybridSearch()

    def test_vector_search(self):
        results = self.search.vector_store.search("ì‚¬ìš©ì í…Œì´ë¸”", top_k=3)
        self.assertGreater(len(results['ids'][0]), 0)

    def test_bm25_search(self):
        results = self.search.bm25_search.search("ì‚¬ìš©ì í…Œì´ë¸”", top_k=3)
        self.assertGreater(len(results), 0)

    def test_hybrid_search(self):
        results = self.search.search("ì‚¬ìš©ì í…Œì´ë¸”", top_k=5)
        self.assertEqual(len(results), 5)
        self.assertIn("users", results[0]['content'])

if __name__ == '__main__':
    unittest.main()
```

### 3.4 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ChromaDB ì„¤ì • ë° í…ŒìŠ¤íŠ¸
- [ ] sentence-transformers ëª¨ë¸ ë¡œë“œ
- [ ] ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] í…Œì´ë¸” ë©”íƒ€ë°ì´í„° Vector DB ì €ì¥
- [ ] BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
- [ ] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬í˜„ (RRF)
- [ ] ê²€ìƒ‰ ì •í™•ë„ í…ŒìŠ¤íŠ¸
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

### 3.5 ì‚°ì¶œë¬¼

- âœ… RAG ì‹œìŠ¤í…œ ì™„ì„±
- âœ… Vector DB êµ¬ì¶•
- âœ… BM25 ê²€ìƒ‰ ì—”ì§„
- âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

---

## 4. Phase 2: Text-to-SQL (1ì£¼)

### 4.1 ëª©í‘œ

ìì—°ì–´ ì§ˆë¬¸ì„ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„

### 4.2 ì‘ì—… ëª©ë¡

#### 4.2.1 Few-shot ì˜ˆì œ ì¤€ë¹„ (1ì¼)

**ì†Œìš” ì‹œê°„**: 8ì‹œê°„

```python
# data/fewshot/examples.json

[
    {
        "question": "ì§€ë‚œë‹¬ ì‹ ê·œ ê°€ì…ì ìˆ˜ëŠ”?",
        "sql_query": "SELECT COUNT(*) FROM users WHERE created_at >= DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month') AND created_at < DATE_TRUNC('month', CURRENT_DATE);",
        "category": "aggregation",
        "difficulty": "easy"
    },
    {
        "question": "ì´ë²ˆ ì£¼ ì£¼ë¬¸ ê¸ˆì•¡ í•©ê³„ëŠ”?",
        "sql_query": "SELECT SUM(total_amount) FROM orders WHERE order_date >= DATE_TRUNC('week', CURRENT_DATE);",
        "category": "aggregation",
        "difficulty": "easy"
    },
    {
        "question": "ì‚¬ìš©ìë³„ í‰ê·  ì£¼ë¬¸ ê¸ˆì•¡ì€?",
        "sql_query": "SELECT u.email, AVG(o.total_amount) as avg_order FROM users u LEFT JOIN orders o ON u.id = o.user_id GROUP BY u.email;",
        "category": "join",
        "difficulty": "medium"
    }
]
```

#### 4.2.2 Text-to-SQL Chain êµ¬í˜„ (3ì¼)

**ì†Œìš” ì‹œê°„**: 24ì‹œê°„

```python
# src/chains/text_to_sql_chain.py

from langchain_openai import AzureChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_community.utilities import SQLDatabase
from typing import Dict
import os

class TextToSQLChain:
    def __init__(self, db_uri):
        self.llm = AzureChatOpenAI(
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            model_name="gpt-4o",
            temperature=0
        )

        self.db = SQLDatabase.from_uri(db_uri)

        self.prompt_template = ChatPromptTemplate.from_messages([
            ("system", """You are an expert SQL assistant.
Generate syntactically correct PostgreSQL queries based on the question and schema.
Return ONLY the SQL query without explanation."""),
            ("user", """Schema:
{schema}

Examples:
{examples}

Question: {question}

SQL:""")
        ])

    def select_relevant_tables(self, question: str, top_k=3) -> list:
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í…Œì´ë¸” ì„ ë³„"""
        # RAG ê²€ìƒ‰ í™œìš©
        from src.rag.hybrid_search import HybridSearch
        search = HybridSearch()
        results = search.search(question, top_k=top_k)

        tables = []
        for result in results:
            if "table_name" in result.get("metadata", {}):
                tables.append(result["metadata"]["table_name"])

        return tables[:top_k]

    def retrieve_examples(self, question: str, top_k=3) -> str:
        """ìœ ì‚¬ ì˜ˆì œ ê²€ìƒ‰"""
        # Few-shot ì˜ˆì œì—ì„œ ê²€ìƒ‰
        import json

        with open("data/fewshot/examples.json") as f:
            examples = json.load(f)

        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ì‹¤ì œë¡œëŠ” ì„ë² ë”© ìœ ì‚¬ë„ ì‚¬ìš©)
        matched_examples = []
        for ex in examples:
            if any(word in question for word in ex["question"].split()):
                matched_examples.append(ex)

        # í¬ë§·íŒ…
        example_str = "\n".join([
            f"Q: {ex['question']}\nSQL: {ex['sql_query']}\n"
            for ex in matched_examples[:top_k]
        ])

        return example_str

    def generate_sql(self, question: str) -> Dict:
        """SQL ìƒì„±"""
        # 1. ê´€ë ¨ í…Œì´ë¸” ì„ ë³„
        relevant_tables = self.select_relevant_tables(question)

        # 2. ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸°
        schema = self.db.get_table_info(relevant_tables)

        # 3. Few-shot ì˜ˆì œ ê²€ìƒ‰
        examples = self.retrieve_examples(question)

        # 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self.prompt_template.format_messages(
            schema=schema,
            examples=examples,
            question=question
        )

        # 5. LLM í˜¸ì¶œ
        response = self.llm.invoke(prompt)

        return {
            "question": question,
            "sql_query": response.content.strip(),
            "selected_tables": relevant_tables
        }
```

#### 4.2.3 ì¿¼ë¦¬ ê²€ì¦ (1ì¼)

**ì†Œìš” ì‹œê°„**: 8ì‹œê°„

```python
# src/chains/query_validator.py

import sqlparse
from sqlparse.sql import IdentifierList, Identifier
from sqlparse.tokens import Keyword, DML

class QueryValidator:
    def __init__(self, db_connection):
        self.conn = db_connection

    def validate(self, query: str) -> Dict:
        """ì¿¼ë¦¬ ê²€ì¦"""

        # 1. ë¬¸ë²• ê²€ì¦
        is_valid, syntax_error = self.validate_syntax(query)
        if not is_valid:
            return {"valid": False, "error": syntax_error}

        # 2. í…Œì´ë¸”/ì¹¼ëŸ¼ ì¡´ì¬ í™•ì¸
        is_valid, existence_error = self.validate_existence(query)
        if not is_valid:
            return {"valid": False, "error": existence_error}

        # 3. ì‹¤í–‰ ê³„íš í™•ì¸
        try:
            cursor = self.conn.cursor()
            cursor.execute(f"EXPLAIN {query}")
            plan = cursor.fetchall()
        except Exception as e:
            return {"valid": False, "error": f"ì‹¤í–‰ ê³„íš ì˜¤ë¥˜: {str(e)}"}

        return {"valid": True, "message": "ê²€ì¦ í†µê³¼"}

    def validate_syntax(self, query: str) -> tuple:
        """ë¬¸ë²• ê²€ì¦"""
        try:
            parsed = sqlparse.parse(query)
            if not parsed:
                return False, "ì¿¼ë¦¬ íŒŒì‹± ì‹¤íŒ¨"

            stmt = parsed[0]
            if stmt.get_type() != 'SELECT':
                return False, "SELECT ì¿¼ë¦¬ë§Œ í—ˆìš©"

            return True, None
        except Exception as e:
            return False, f"ë¬¸ë²• ì˜¤ë¥˜: {str(e)}"

    def validate_existence(self, query: str) -> tuple:
        """í…Œì´ë¸”/ì¹¼ëŸ¼ ì¡´ì¬ í™•ì¸"""
        # ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
        return True, None
```

#### 4.2.4 í†µí•© í…ŒìŠ¤íŠ¸ (2ì¼)

**ì†Œìš” ì‹œê°„**: 16ì‹œê°„

```python
# tests/test_text_to_sql.py

import unittest
from src.chains.text_to_sql_chain import TextToSQLChain
from src.chains.query_validator import QueryValidator

class TestTextToSQL(unittest.TestCase):
    def setUp(self):
        self.chain = TextToSQLChain("postgresql://user:pass@localhost/db")
        self.validator = QueryValidator(self.chain.db._engine.raw_connection())

    def test_simple_aggregation(self):
        result = self.chain.generate_sql("ì§€ë‚œë‹¬ ì‹ ê·œ ê°€ì…ì ìˆ˜ëŠ”?")
        self.assertIn("SELECT", result["sql_query"])
        self.assertIn("COUNT", result["sql_query"])
        self.assertIn("users", result["sql_query"].lower())

        # ê²€ì¦
        validation = self.validator.validate(result["sql_query"])
        self.assertTrue(validation["valid"])

    def test_join_query(self):
        result = self.chain.generate_sql("ì‚¬ìš©ìë³„ ì£¼ë¬¸ ê±´ìˆ˜ëŠ”?")
        self.assertIn("JOIN", result["sql_query"])
        self.assertIn("GROUP BY", result["sql_query"])

    def test_date_filtering(self):
        result = self.chain.generate_sql("ì´ë²ˆ ì£¼ ì£¼ë¬¸ ëª©ë¡ì€?")
        self.assertIn("DATE_TRUNC", result["sql_query"] or result["sql_query"])
```

### 4.3 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Few-shot ì˜ˆì œ ì‘ì„± (ìµœì†Œ 10ê°œ)
- [ ] Text-to-SQL Chain êµ¬í˜„
- [ ] í…Œì´ë¸” ìë™ ì„ ë³„ ê¸°ëŠ¥
- [ ] ì˜ˆì œ ê²€ìƒ‰ ê¸°ëŠ¥
- [ ] ì¿¼ë¦¬ ê²€ì¦ ì‹œìŠ¤í…œ
- [ ] ë¬¸ë²• ê²€ì¦ êµ¬í˜„
- [ ] ì‹¤í–‰ ê³„íš ë¶„ì„
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### 4.4 ì‚°ì¶œë¬¼

- âœ… Text-to-SQL ì—”ì§„
- âœ… ì¿¼ë¦¬ ê²€ì¦ ì‹œìŠ¤í…œ
- âœ… Few-shot ì˜ˆì œ DB

---

## 5. Phase 3: ì¶”ê°€ ê¸°ëŠ¥ (1ì£¼)

### 5.1 Data Discovery êµ¬í˜„ (2ì¼)

```python
# src/chains/data_discovery_chain.py

class DataDiscoveryChain:
    def explain_table(self, table_name: str) -> str:
        """í…Œì´ë¸” ì„¤ëª…"""
        # ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        # ì¹¼ëŸ¼ ì •ë³´ ì¡°íšŒ
        # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
        # LLMìœ¼ë¡œ ìì—°ì–´ ì„¤ëª… ìƒì„±
        pass

    def explain_column(self, table_name: str, column_name: str) -> str:
        """ì¹¼ëŸ¼ ì„¤ëª…"""
        pass
```

### 5.2 Memory Management êµ¬í˜„ (2ì¼)

```python
# src/memory/conversation_memory.py

import sqlite3
from datetime import datetime

class ConversationMemory:
    def __init__(self, db_path="memory.db"):
        self.conn = sqlite3.connect(db_path)
        self._create_table()

    def _create_table(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS conversations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                turn_number INTEGER,
                user_message TEXT,
                assistant_message TEXT,
                generated_query TEXT,
                timestamp TIMESTAMP
            )
        """)

    def save_conversation(self, session_id, turn, user_msg, ai_msg, query=None):
        self.conn.execute("""
            INSERT INTO conversations
            (session_id, turn_number, user_message, assistant_message, generated_query, timestamp)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (session_id, turn, user_msg, ai_msg, query, datetime.now()))
        self.conn.commit()

    def get_recent_conversations(self, session_id, limit=5):
        cursor = self.conn.execute("""
            SELECT user_message, assistant_message, generated_query
            FROM conversations
            WHERE session_id = ?
            ORDER BY turn_number DESC
            LIMIT ?
        """, (session_id, limit))
        return cursor.fetchall()
```

### 5.3 Knowledge Discovery êµ¬í˜„ (3ì¼)

```python
# src/chains/knowledge_discovery_chain.py

from langchain.chains import RetrievalQA

class KnowledgeDiscoveryChain:
    def __init__(self, vector_store):
        self.vector_store = vector_store
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=vector_store.as_retriever(search_kwargs={"k": 5})
        )

    def answer_question(self, question: str) -> str:
        """ì§€ì‹ ê²€ìƒ‰ ë° ë‹µë³€"""
        response = self.qa_chain.invoke({"query": question})
        return response["result"]
```

---

## 6. Phase 4: UI êµ¬ì¶• (1ì£¼)

### 6.1 Streamlit UI êµ¬í˜„ (5ì¼)

**íŒŒì¼**: `src/ui/app.py`

```python
import streamlit as st
from src.chains.text_to_sql_chain import TextToSQLChain
import uuid

# ì„¸ì…˜ ì´ˆê¸°í™”
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
if "messages" not in st.session_state:
    st.session_state.messages = []

# UI ë ˆì´ì•„ì›ƒ
st.title("ğŸ¤– AI ë°ì´í„° ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ì„¤ì •")
    api_key = st.text_input("OpenAI API Key", type="password")
    db_uri = st.text_input("Database URI")

    if st.button("ì—°ê²° í…ŒìŠ¤íŠ¸"):
        try:
            chain = TextToSQLChain(db_uri)
            st.success("âœ… ì—°ê²° ì„±ê³µ!")
        except Exception as e:
            st.error(f"âŒ ì—°ê²° ì‹¤íŒ¨: {str(e)}")

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            chain = TextToSQLChain(db_uri)
            result = chain.generate_sql(prompt)

            response = f"""
**ìƒì„±ëœ ì¿¼ë¦¬**:
```sql
{result['sql_query']}
```

**ì„ íƒëœ í…Œì´ë¸”**: {', '.join(result['selected_tables'])}
"""

            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
```

### 6.2 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Streamlit ì•± ê¸°ë³¸ êµ¬ì¡°
- [ ] ëŒ€í™” ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
- [ ] SQL ì¿¼ë¦¬ í‘œì‹œ
- [ ] ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
- [ ] ì‚¬ì´ë“œë°” ì„¤ì • UI
- [ ] ì„¸ì…˜ ê´€ë¦¬
- [ ] í”¼ë“œë°± ë²„íŠ¼

---

## 7. Phase 5: í…ŒìŠ¤íŒ… (1ì£¼)

### 7.1 í…ŒìŠ¤íŠ¸ ê³„íš

| í…ŒìŠ¤íŠ¸ ìœ í˜• | ì»¤ë²„ë¦¬ì§€ | ë„êµ¬ |
|------------|----------|------|
| ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ | 80% | pytest |
| í†µí•© í…ŒìŠ¤íŠ¸ | ì£¼ìš” ê¸°ëŠ¥ | pytest |
| E2E í…ŒìŠ¤íŠ¸ | UI í”Œë¡œìš° | Streamlit test |
| ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ | ì‘ë‹µ ì‹œê°„ | pytest-benchmark |

### 7.2 í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

```python
# tests/test_e2e.py

def test_end_to_end_text_to_sql():
    # 1. ì§ˆë¬¸ ì…ë ¥
    question = "ì§€ë‚œë‹¬ ì‹ ê·œ ê°€ì…ì ìˆ˜ëŠ”?"

    # 2. SQL ìƒì„±
    chain = TextToSQLChain(db_uri)
    result = chain.generate_sql(question)

    # 3. ê²€ì¦
    validator = QueryValidator(db_connection)
    validation = validator.validate(result["sql_query"])
    assert validation["valid"]

    # 4. ì‹¤í–‰
    cursor = db_connection.cursor()
    cursor.execute(result["sql_query"])
    rows = cursor.fetchall()
    assert len(rows) > 0
```

---

## 8. Phase 6: ë°°í¬ (ì„ íƒ)

### 8.1 Docker ë°°í¬

**docker-compose.yml**:
```yaml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8501:8501"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DATABASE_URL=${DATABASE_URL}
    volumes:
      - ./chromadb_data:/app/chromadb_data

  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=yourdb
      - POSTGRES_PASSWORD=yourpass
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

volumes:
  postgres_data:
```

**ì‹¤í–‰**:
```bash
docker-compose up -d
```

---

## 9. ë¦¬ìŠ¤í¬ ê´€ë¦¬

### 9.1 ê¸°ìˆ  ë¦¬ìŠ¤í¬

| ë¦¬ìŠ¤í¬ | í™•ë¥  | ì˜í–¥ | ì™„í™” ë°©ì•ˆ |
|--------|------|------|----------|
| LLM API ë¹„ìš© ì´ˆê³¼ | ì¤‘ | ê³  | ìºì‹±, í”„ë¡¬í”„íŠ¸ ì••ì¶• |
| ì¿¼ë¦¬ ì •í™•ë„ ë¶€ì¡± | ê³  | ê³  | Few-shot ì˜ˆì œ í™•ì¶©, ê²€ì¦ ê°•í™” |
| ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ | ì¤‘ | ì¤‘ | BM25 ì‚¬ì „ ê³„ì‚°, ë³‘ë ¬ ì²˜ë¦¬ |
| ChromaDB ì„±ëŠ¥ ì €í•˜ | ì € | ì¤‘ | HNSW íŒŒë¼ë¯¸í„° ìµœì í™” |

### 9.2 ì¼ì • ë¦¬ìŠ¤í¬

**ë²„í¼**: ê° Phaseì— 20% ì—¬ìœ  ì‹œê°„ í¬í•¨

**ìš°ì„ ìˆœìœ„**:
1. Text-to-SQL (í•µì‹¬)
2. RAG ì‹œìŠ¤í…œ (í•µì‹¬)
3. UI (ì¤‘ìš”)
4. ì¶”ê°€ ê¸°ëŠ¥ (ì„ íƒ)

---

## 10. í’ˆì§ˆ ê´€ë¦¬

### 10.1 ì½”ë“œ í’ˆì§ˆ

- **Linting**: flake8, black
- **Type Checking**: mypy
- **Test Coverage**: >80%

### 10.2 ë¬¸ì„œí™”

- **README.md**: í”„ë¡œì íŠ¸ ì†Œê°œ, ì„¤ì¹˜, ì‚¬ìš©ë²•
- **API ë¬¸ì„œ**: Docstring (Sphinx)
- **ì‚¬ìš©ì ê°€ì´ë“œ**: Streamlit UI ì‚¬ìš©ë²•

### 10.3 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

- ì‘ë‹µ ì‹œê°„ ì¶”ì 
- LLM API ë¹„ìš© ì¶”ì 
- ì¿¼ë¦¬ ì •í™•ë„ ì¶”ì 

---

**ë¬¸ì„œ ë²„ì „**: 1.0.0
**ìµœì¢… ìˆ˜ì •**: 2025-01-13
**ìŠ¹ì¸ì**: -
