"""
Embedding ÌíàÏßà Í≤ÄÏ¶ù Ïä§ÌÅ¨Î¶ΩÌä∏

ÏûÑÎ≤†Îî© Î™®Îç∏Ïùò Í≤ÄÏÉâ Ï†ïÌôïÎèÑÏôÄ ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌï©ÎãàÎã§.
"""

import sys
import time
from pathlib import Path
from typing import Dict, List, Tuple

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.services.embedding import HuggingFaceEmbedding
from src.utils.logging import logger


# ÌèâÍ∞ÄÏö© ÌÖåÏä§Ìä∏ ÏøºÎ¶¨ Î∞è Í∏∞ÎåÄ Í≤∞Í≥º
TEST_QUERIES = [
    {
        "query": "ÌöåÏõêÍ∞ÄÏûÖÌïòÎäî Î∞©Î≤ïÏùÑ ÏïåÎ†§Ï£ºÏÑ∏Ïöî",
        "expected_doc_ids": ["doc_001"],  # ÌöåÏõêÍ∞ÄÏûÖ Í∞ÄÏù¥Îìú
        "expected_categories": ["Í≥ÑÏ†ïÍ¥ÄÎ¶¨"],
        "min_relevance": 0.7,
    },
    {
        "query": "ÎπÑÎ∞ÄÎ≤àÌò∏Î•º ÏûäÏñ¥Î≤ÑÎ†∏Ïñ¥Ïöî",
        "expected_doc_ids": ["doc_002"],  # ÎπÑÎ∞ÄÎ≤àÌò∏ Ïû¨ÏÑ§Ï†ï Î∞©Î≤ï
        "expected_categories": ["Í≥ÑÏ†ïÍ¥ÄÎ¶¨"],
        "min_relevance": 0.7,
    },
    {
        "query": "Ï£ºÎ¨∏ Ï∑®ÏÜåÌïòÍ≥† Î∞òÌíàÌïòÎ†§Î©¥ Ïñ¥ÎñªÍ≤å Ìï¥Ïïº ÌïòÎÇòÏöî?",
        "expected_doc_ids": ["doc_004"],  # Î∞òÌíà Î∞è ÍµêÌôò Ï†ïÏ±Ö
        "expected_categories": ["Î∞òÌíà/ÍµêÌôò"],
        "min_relevance": 0.6,
    },
    {
        "query": "ÏÉÅÌíà Í≤∞Ï†ú Î∞©Î≤ïÏù¥ Í∂ÅÍ∏àÌï©ÎãàÎã§",
        "expected_doc_ids": ["doc_003"],  # Ï£ºÎ¨∏ Î∞è Í≤∞Ï†ú Í∞ÄÏù¥Îìú
        "expected_categories": ["Ï£ºÎ¨∏/Í≤∞Ï†ú"],
        "min_relevance": 0.6,
    },
    {
        "query": "Í≥†Í∞ùÏÑºÌÑ∞Ïóê Î¨∏ÏùòÌïòÍ≥† Ïã∂Ïñ¥Ïöî",
        "expected_doc_ids": ["doc_005"],  # Í≥†Í∞ùÏÑºÌÑ∞ Ïù¥Ïö© ÏïàÎÇ¥
        "expected_categories": ["Í≥†Í∞ùÏßÄÏõê"],
        "min_relevance": 0.7,
    },
    {
        "query": "Ïù¥Î©îÏùº Ïù∏Ï¶ùÏùÄ Ïñ¥ÎñªÍ≤å ÌïòÎÇòÏöî?",
        "expected_doc_ids": ["doc_001"],  # ÌöåÏõêÍ∞ÄÏûÖ Í∞ÄÏù¥Îìú (Ïù¥Î©îÏùº Ïù∏Ï¶ù Ìè¨Ìï®)
        "expected_categories": ["Í≥ÑÏ†ïÍ¥ÄÎ¶¨"],
        "min_relevance": 0.5,
    },
    {
        "query": "Ïã†Ïö©Ïπ¥ÎìúÎ°ú Í≤∞Ï†úÌï† Ïàò ÏûàÎÇòÏöî?",
        "expected_doc_ids": ["doc_003"],  # Ï£ºÎ¨∏ Î∞è Í≤∞Ï†ú Í∞ÄÏù¥Îìú
        "expected_categories": ["Ï£ºÎ¨∏/Í≤∞Ï†ú"],
        "min_relevance": 0.5,
    },
]


class EmbeddingQualityValidator:
    """ÏûÑÎ≤†Îî© ÌíàÏßà Í≤ÄÏ¶ù ÌÅ¥ÎûòÏä§"""

    def __init__(self):
        """ÏûÑÎ≤†Îî© ÏÑúÎπÑÏä§ Ï¥àÍ∏∞Ìôî"""
        self.embedding_service = HuggingFaceEmbedding()
        self.results: List[Dict] = []

    def validate_retrieval_accuracy(self, top_k: int = 3) -> Dict:
        """
        Í≤ÄÏÉâ Ï†ïÌôïÎèÑ ÌèâÍ∞Ä

        Args:
            top_k: ÏÉÅÏúÑ Î™á Í∞úÏùò Î¨∏ÏÑúÎ•º Í≤ÄÏÉâÌï†ÏßÄ

        Returns:
            Ï†ïÌôïÎèÑ ÌèâÍ∞Ä Í≤∞Í≥º
        """
        print("=" * 80)
        print(f" Retrieval Accuracy Validation (top_k={top_k})")
        print("=" * 80)
        print()

        total_queries = len(TEST_QUERIES)
        correct_retrievals = 0
        relevance_scores = []

        for i, test_case in enumerate(TEST_QUERIES, 1):
            query = test_case["query"]
            expected_doc_ids = test_case["expected_doc_ids"]
            expected_categories = test_case["expected_categories"]
            min_relevance = test_case["min_relevance"]

            print(f"Test {i}/{total_queries}: {query}")

            # Perform search
            start_time = time.time()
            results = self.embedding_service.search(query_text=query, top_k=top_k)
            search_time = time.time() - start_time

            if not results or not results.get("documents") or not results["documents"][0]:
                print("  ‚ùå No results found")
                self.results.append(
                    {
                        "query": query,
                        "success": False,
                        "reason": "No results",
                        "search_time": search_time,
                    }
                )
                print()
                continue

            # Check if expected document is in top results
            retrieved_ids = results["ids"][0]
            retrieved_metadatas = results["metadatas"][0]
            distances = results["distances"][0] if results.get("distances") else []

            # Calculate relevance scores (1 - distance)
            relevance_scores_query = [1.0 - d for d in distances] if distances else []

            # Check if any expected document is retrieved
            found_expected = any(doc_id in retrieved_ids for doc_id in expected_doc_ids)

            # Check category match
            retrieved_categories = [
                meta.get("category", "") for meta in retrieved_metadatas
            ]
            category_match = any(cat in retrieved_categories for cat in expected_categories)

            # Check relevance threshold
            max_relevance = max(relevance_scores_query) if relevance_scores_query else 0.0
            relevance_ok = max_relevance >= min_relevance

            # Overall success
            success = found_expected and relevance_ok

            if success:
                correct_retrievals += 1
                status = "‚úÖ"
            else:
                status = "‚ö†Ô∏è"

            print(f"  {status} Expected: {expected_doc_ids[0]} (min relevance: {min_relevance:.2f})")
            print(f"     Retrieved:")
            for j, (doc_id, metadata, score) in enumerate(
                zip(retrieved_ids, retrieved_metadatas, relevance_scores_query), 1
            ):
                title = metadata.get("title", "Unknown")
                category = metadata.get("category", "Unknown")
                match_indicator = "‚úì" if doc_id in expected_doc_ids else " "
                print(f"       {j}. [{match_indicator}] {title} ({category}) - score: {score:.3f}")

            print(f"     Search time: {search_time:.3f}s")
            print(f"     Category match: {category_match}")

            self.results.append(
                {
                    "query": query,
                    "success": success,
                    "expected_ids": expected_doc_ids,
                    "retrieved_ids": retrieved_ids,
                    "max_relevance": max_relevance,
                    "category_match": category_match,
                    "search_time": search_time,
                }
            )

            if relevance_scores_query:
                relevance_scores.extend(relevance_scores_query)

            print()

        # Calculate overall metrics
        accuracy = correct_retrievals / total_queries
        avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.0
        avg_search_time = sum(r["search_time"] for r in self.results) / len(self.results)

        print("-" * 80)
        print(" Summary")
        print("-" * 80)
        print(f"Accuracy: {correct_retrievals}/{total_queries} ({accuracy:.1%})")
        print(f"Average Relevance Score: {avg_relevance:.3f}")
        print(f"Average Search Time: {avg_search_time:.3f}s")
        print()

        return {
            "accuracy": accuracy,
            "correct_retrievals": correct_retrievals,
            "total_queries": total_queries,
            "avg_relevance": avg_relevance,
            "avg_search_time": avg_search_time,
        }

    def benchmark_performance(self, num_iterations: int = 10) -> Dict:
        """
        ÏûÑÎ≤†Îî© Î∞è Í≤ÄÏÉâ ÏÑ±Îä• Î≤§ÏπòÎßàÌÅ¨

        Args:
            num_iterations: Î∞òÎ≥µ ÌöüÏàò

        Returns:
            ÏÑ±Îä• Î≤§ÏπòÎßàÌÅ¨ Í≤∞Í≥º
        """
        print("=" * 80)
        print(f" Performance Benchmark ({num_iterations} iterations)")
        print("=" * 80)
        print()

        # Test embedding speed
        print("üîÑ Testing embedding speed...")
        test_texts = [tc["query"] for tc in TEST_QUERIES]
        embedding_times = []

        for i in range(num_iterations):
            start_time = time.time()
            embeddings = self.embedding_service.embed_texts(test_texts)
            embedding_time = time.time() - start_time
            embedding_times.append(embedding_time)

        avg_embedding_time = sum(embedding_times) / len(embedding_times)
        texts_per_second = len(test_texts) / avg_embedding_time

        print(f"‚úÖ Embedding speed:")
        print(f"   Average time: {avg_embedding_time:.3f}s for {len(test_texts)} texts")
        print(f"   Throughput: {texts_per_second:.1f} texts/second")
        print()

        # Test search speed
        print("üîÑ Testing search speed...")
        search_times = []

        for i in range(num_iterations):
            start_time = time.time()
            for test_case in TEST_QUERIES:
                self.embedding_service.search(query_text=test_case["query"], top_k=3)
            search_time = time.time() - start_time
            search_times.append(search_time)

        avg_search_time = sum(search_times) / len(search_times)
        searches_per_second = len(TEST_QUERIES) / avg_search_time

        print(f"‚úÖ Search speed:")
        print(f"   Average time: {avg_search_time:.3f}s for {len(TEST_QUERIES)} searches")
        print(f"   Throughput: {searches_per_second:.1f} searches/second")
        print()

        return {
            "embedding": {
                "avg_time": avg_embedding_time,
                "texts_per_second": texts_per_second,
            },
            "search": {
                "avg_time": avg_search_time,
                "searches_per_second": searches_per_second,
            },
        }

    def validate_model_info(self) -> Dict:
        """
        ÏûÑÎ≤†Îî© Î™®Îç∏ Ï†ïÎ≥¥ Í≤ÄÏ¶ù

        Returns:
            Î™®Îç∏ Ï†ïÎ≥¥
        """
        print("=" * 80)
        print(" Embedding Model Information")
        print("=" * 80)
        print()

        model_name = self.embedding_service.model_name
        embedding_dim = self.embedding_service.embedding_dim
        device = self.embedding_service.device

        print(f"Model Name: {model_name}")
        print(f"Embedding Dimensions: {embedding_dim}")
        print(f"Device: {device}")
        print()

        # Test single embedding
        test_text = "ÌÖåÏä§Ìä∏ ÌÖçÏä§Ìä∏ÏûÖÎãàÎã§"
        embedding = self.embedding_service.embed_text(test_text)

        print(f"Sample Embedding:")
        print(f"  Input: '{test_text}'")
        print(f"  Output shape: {len(embedding)}")
        print(f"  First 5 values: {embedding[:5]}")
        print()

        return {
            "model_name": model_name,
            "embedding_dim": embedding_dim,
            "device": device,
        }


def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    print("\n" + "=" * 80)
    print(" Embedding Quality Validation")
    print(" Hugging Face sentence-transformers")
    print("=" * 80 + "\n")

    try:
        validator = EmbeddingQualityValidator()

        # 1. Model information
        model_info = validator.validate_model_info()

        # 2. Retrieval accuracy
        accuracy_results = validator.validate_retrieval_accuracy(top_k=3)

        # 3. Performance benchmark
        performance_results = validator.benchmark_performance(num_iterations=5)

        # Final report
        print("=" * 80)
        print(" Final Report")
        print("=" * 80)
        print()
        print(f"Model: {model_info['model_name']}")
        print(f"Dimensions: {model_info['embedding_dim']}")
        print(f"Device: {model_info['device']}")
        print()
        print(f"Retrieval Accuracy: {accuracy_results['accuracy']:.1%}")
        print(f"Average Relevance: {accuracy_results['avg_relevance']:.3f}")
        print(f"Average Search Time: {accuracy_results['avg_search_time']:.3f}s")
        print()
        print(f"Embedding Speed: {performance_results['embedding']['texts_per_second']:.1f} texts/s")
        print(f"Search Speed: {performance_results['search']['searches_per_second']:.1f} searches/s")
        print()

        # Quality assessment
        if accuracy_results["accuracy"] >= 0.8:
            print("‚úÖ PASS: Embedding quality is good (‚â•80% accuracy)")
        elif accuracy_results["accuracy"] >= 0.6:
            print("‚ö†Ô∏è WARNING: Embedding quality needs improvement (60-80% accuracy)")
        else:
            print("‚ùå FAIL: Embedding quality is poor (<60% accuracy)")

        print("\n" + "=" * 80 + "\n")

    except Exception as e:
        logger.error(f"Validation failed: {e}")
        print(f"\n‚ùå Error: {e}\n")
        raise


if __name__ == "__main__":
    main()
