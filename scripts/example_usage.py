"""
AI Assistant ì‚¬ìš© ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ê° ì²´ì¸ì˜ ì‹¤ì œ ì‚¬ìš©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.chains.router import RouterChain
from src.chains.text_to_sql import TextToSQLChain
from src.chains.knowledge import KnowledgeChain
from src.chains.multi_turn import MultiTurnChain
from src.models.query_response import QueryRequest, QueryType
from src.services.llm_client import LLMClient
from src.services.embedding import HuggingFaceEmbedding
from src.services.memory import SQLiteConversationMemory
from src.utils.logging import logger


def print_section(title: str):
    """Print formatted section header."""
    print("\n" + "=" * 80)
    print(f" {title}")
    print("=" * 80 + "\n")


def example_router_chain():
    """Example: Intent Classification using Router Chain."""
    print_section("1. Intent Classification (Router Chain)")

    llm_client = LLMClient()
    router = RouterChain(llm_client)

    # Test queries in Korean
    test_queries = [
        ("ì§€ë‚œë‹¬ ì‹ ê·œ ê°€ì…ì ìˆ˜ëŠ”?", QueryType.TEXT_TO_SQL),
        ("íšŒì›ê°€ì… ì ˆì°¨ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", QueryType.KNOWLEDGE),
        ("ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”", QueryType.ASSISTANT),
        ("ìµœê·¼ 1ì£¼ì¼ê°„ ì¼ë³„ ì£¼ë¬¸ ê¸ˆì•¡ì„ ì¡°íšŒí•´ì£¼ì„¸ìš”", QueryType.TEXT_TO_SQL),
        ("ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •í•˜ëŠ” ë°©ë²• ì•Œë ¤ì£¼ì„¸ìš”", QueryType.KNOWLEDGE),
    ]

    for query_text, expected_type in test_queries:
        query = QueryRequest(user_id="example_user", query_text=query_text)

        try:
            query_type = router.classify(query)
            status = "âœ…" if query_type == expected_type else "âš ï¸"
            print(f"{status} Query: {query_text}")
            print(f"   Classified as: {query_type.value}")
            print(f"   Expected: {expected_type.value}")
            print()
        except Exception as e:
            print(f"âŒ Error classifying query: {e}\n")


def example_text_to_sql_chain():
    """Example: Text-to-SQL generation."""
    print_section("2. Text-to-SQL Generation")

    llm_client = LLMClient()
    text_to_sql = TextToSQLChain(llm_client)

    # Test SQL generation queries
    sql_queries = [
        "ì§€ë‚œ 7ì¼ê°„ ì¼ë³„ ì‹ ê·œ ê°€ì…ì ìˆ˜ë¥¼ ì¡°íšŒí•´ì£¼ì„¸ìš”",
        "ì´ë²ˆ ë‹¬ ì´ ì£¼ë¬¸ ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?",
        "ê°€ì¥ ë§ì´ íŒ”ë¦° ìƒí’ˆ 10ê°œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
    ]

    for query_text in sql_queries:
        query = QueryRequest(user_id="example_user", query_text=query_text)

        try:
            response = text_to_sql.generate_sql(query)
            print(f"Query: {query_text}")
            print(f"Generated SQL:")
            print("-" * 60)
            print(response.sql_query)
            print("-" * 60)
            print(f"Confidence: {response.confidence_score:.2f}")
            print(f"Token Usage: {response.token_usage.total_tokens} tokens")
            print()
        except Exception as e:
            print(f"âŒ Error generating SQL: {e}\n")


def example_knowledge_chain():
    """Example: RAG-based Knowledge Discovery."""
    print_section("3. Knowledge Discovery (RAG)")

    llm_client = LLMClient()
    embedding_service = HuggingFaceEmbedding()
    knowledge_chain = KnowledgeChain(llm_client, embedding_service)

    # Test knowledge queries
    knowledge_queries = [
        "íšŒì›ê°€ì…í•  ë•Œ ì´ë©”ì¼ ì¸ì¦ì´ í•„ìš”í•œê°€ìš”?",
        "ë¹„ë°€ë²ˆí˜¸ëŠ” ì–´ë–¤ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•˜ë‚˜ìš”?",
        "ê³„ì •ì„ ì‚­ì œí•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
    ]

    for query_text in knowledge_queries:
        query = QueryRequest(user_id="example_user", query_text=query_text)

        try:
            response = knowledge_chain.search(query, top_k=3)
            print(f"Query: {query_text}")
            print(f"Answer:")
            print("-" * 60)
            print(response.answer)
            print("-" * 60)
            print(f"Confidence: {response.confidence_score:.2f}")
            print(f"Source Documents: {len(response.source_documents)}")
            for i, doc in enumerate(response.source_documents, 1):
                print(f"  {i}. {doc.title} (relevance: {doc.relevance_score:.2f})")
            print(f"Token Usage: {response.token_usage.total_tokens} tokens")
            print()
        except Exception as e:
            print(f"âŒ Error searching knowledge: {e}\n")


def example_multi_turn_chat():
    """Example: Multi-turn conversation with history."""
    print_section("4. Multi-turn Conversation")

    llm_client = LLMClient()
    memory = SQLiteConversationMemory()
    chat = MultiTurnChain(llm_client, memory)

    session_id = "example_session_001"

    # Conversation turns
    conversation = [
        "ì•ˆë…•í•˜ì„¸ìš”!",
        "ì£¼ë¬¸ ë‚´ì—­ì„ í™•ì¸í•˜ê³  ì‹¶ì–´ìš”",
        "ì§€ë‚œë‹¬ ì£¼ë¬¸ ë‚´ì—­ì´ìš”",
        "ê°ì‚¬í•©ë‹ˆë‹¤!",
    ]

    for i, query_text in enumerate(conversation, 1):
        query = QueryRequest(
            user_id="example_user", session_id=session_id, query_text=query_text
        )

        try:
            response = chat.chat(query)
            print(f"Turn {i}:")
            print(f"User: {query_text}")
            print(f"Assistant: {response.answer}")
            print(f"Confidence: {response.confidence_score:.2f}")
            print(f"Token Usage: {response.token_usage.total_tokens} tokens")
            print()
        except Exception as e:
            print(f"âŒ Error in conversation: {e}\n")

    # Display conversation history
    print("-" * 60)
    print("Conversation History:")
    print("-" * 60)
    history = memory.get_conversation_history(session_id, limit=10)
    for i, turn in enumerate(history, 1):
        print(f"Turn {i}:")
        print(f"  User: {turn['user_message']}")
        print(f"  Assistant: {turn['assistant_message']}")
        print()


def main():
    """Run all examples."""
    print("\n" + "=" * 80)
    print(" AI Assistant - Usage Examples")
    print(" Claude Code + Hugging Face Embeddings")
    print("=" * 80)

    try:
        # Check LLM connection
        print("\nğŸ”„ Testing Claude API connection...")
        llm_client = LLMClient()
        if llm_client.test_connection():
            print("âœ… Claude API connection successful!\n")
        else:
            print("âŒ Claude API connection failed. Check your API key.\n")
            return

        # Run examples
        example_router_chain()
        example_text_to_sql_chain()
        example_knowledge_chain()
        example_multi_turn_chat()

        print("\n" + "=" * 80)
        print(" All examples completed!")
        print("=" * 80 + "\n")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ Examples interrupted by user.\n")
    except Exception as e:
        logger.error(f"Error running examples: {e}")
        print(f"\nâŒ Error: {e}\n")


if __name__ == "__main__":
    main()
