# êµ¬í˜„ ê°€ì´ë“œ (ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜)

## ğŸ¯ êµ¬í˜„ ê°œìš”

ì´ ê°€ì´ë“œëŠ” **ì™„ì „ ë¬´ë£Œ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ìˆ  ìŠ¤íƒ**ìœ¼ë¡œ AI ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œì„ ë‹¨ê³„ë³„ë¡œ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

**ì˜ˆìƒ êµ¬í˜„ ì‹œê°„**: 4-6ì£¼ (1ì¸ ê¸°ì¤€)
**í•„ìˆ˜ ì˜ˆì‚°**: LLM API ë¹„ìš©ë§Œ (ì›” $50-200)
**ì¸í”„ë¼ ë¹„ìš©**: $0 (ëª¨ë‘ ì˜¤í”ˆì†ŒìŠ¤)

---

## ğŸ“‹ Phase 0: í™˜ê²½ ì„¤ì •

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

**ìµœì†Œ ì‚¬ì–‘**:
- CPU: 4ì½”ì–´ ì´ìƒ
- RAM: 8GB ì´ìƒ
- ì €ì¥ê³µê°„: 20GB ì´ìƒ
- Python: 3.10 ì´ìƒ

**ê¶Œì¥ ì‚¬ì–‘**:
- CPU: 8ì½”ì–´ ì´ìƒ
- RAM: 16GB ì´ìƒ (ì„ë² ë”© ëª¨ë¸ ë¡œì»¬ ì‹¤í–‰)
- GPU: NVIDIA GPU with CUDA (ì„ íƒ, 10ë°° ë¹ ë¥¸ ì„ë² ë”©)
- ì €ì¥ê³µê°„: 50GB ì´ìƒ

### 1. Python í™˜ê²½ êµ¬ì¶•

```bash
# Python 3.10+ ì„¤ì¹˜ í™•ì¸
python --version  # 3.10 ì´ìƒ í•„ìš”

# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv

# í™œì„±í™”
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate  # Windows
```

### 2. PostgreSQL ì„¤ì¹˜

**macOS**:
```bash
brew install postgresql@15
brew services start postgresql@15
createdb ai_data_tool
```

**Ubuntu/Debian**:
```bash
sudo apt update
sudo apt install postgresql postgresql-contrib
sudo systemctl start postgresql
sudo -u postgres createdb ai_data_tool
```

**Windows**:
```bash
# PostgreSQL Installer ë‹¤ìš´ë¡œë“œ
# https://www.postgresql.org/download/windows/
# ì„¤ì¹˜ í›„:
createdb ai_data_tool
```

### 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜

**requirements.txt ìƒì„±**:
```txt
# LLM & Framework
langchain==0.1.0
langchain-community==0.0.13
langchain-openai==0.0.5
langgraph==0.0.20
openai==1.10.0

# Vector DB & Search
chromadb==0.4.22
sentence-transformers==2.3.1
rank-bm25==0.2.2

# Database
psycopg2-binary==2.9.9
sqlalchemy==2.0.25

# Document Processing
PyPDF2==3.0.1
python-docx==1.1.0
markdown==3.5.2
pytesseract==0.3.10

# Web UI
streamlit==1.30.0

# Utilities
python-dotenv==1.0.0
requests==2.31.0
numpy==1.24.3
```

**ì„¤ì¹˜**:
```bash
pip install -r requirements.txt
```

### 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

**.env íŒŒì¼ ìƒì„±**:
```env
# OpenAI API
OPENAI_API_KEY=your-api-key-here

# Database
DATABASE_URL=postgresql://localhost:5432/ai_data_tool

# ChromaDB
CHROMA_DB_PATH=./chromadb_data

# SQLite
SQLITE_DB_PATH=./sqlite_data

# Streamlit
STREAMLIT_SERVER_PORT=8501
```

### 5. ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±

```bash
mkdir -p ai-data-tool/{src,data,chromadb_data,sqlite_data,logs,docs}
cd ai-data-tool

# ì†ŒìŠ¤ ë””ë ‰í† ë¦¬
mkdir -p src/{rag,chains,agents,ui,utils}

# ë°ì´í„° ë””ë ‰í† ë¦¬
mkdir -p data/{documents,metadata,few_shot}

# ë¡œê·¸ ë””ë ‰í† ë¦¬
mkdir -p logs
```

**ìµœì¢… êµ¬ì¡°**:
```
ai-data-tool/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag/              # RAG ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ chains/           # LangChain ì²´ì¸
â”‚   â”œâ”€â”€ agents/           # ì—ì´ì „íŠ¸
â”‚   â”œâ”€â”€ ui/               # Streamlit UI
â”‚   â””â”€â”€ utils/            # ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/        # ì›ë³¸ ë¬¸ì„œ
â”‚   â”œâ”€â”€ metadata/         # ë©”íƒ€ë°ì´í„°
â”‚   â””â”€â”€ few_shot/         # Few-shot ì˜ˆì œ
â”œâ”€â”€ chromadb_data/        # ChromaDB ì €ì¥ì†Œ
â”œâ”€â”€ sqlite_data/          # SQLite DB
â”œâ”€â”€ logs/                 # ë¡œê·¸ íŒŒì¼
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## ğŸ“‹ Phase 1: RAG ì‹œìŠ¤í…œ êµ¬ì¶• (1ì£¼)

### 1.1 ChromaDB ì´ˆê¸°í™”

**src/rag/vector_store.py**:
```python
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import os

class VectorStore:
    def __init__(self, persist_directory="./chromadb_data"):
        # ChromaDB í´ë¼ì´ì–¸íŠ¸
        self.client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=persist_directory
        ))

        # ì„ë² ë”© ëª¨ë¸
        self.embed_model = SentenceTransformer(
            'paraphrase-multilingual-MiniLM-L12-v2'
        )

        # ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self._init_collections()

    def _init_collections(self):
        """ì»¬ë ‰ì…˜ ìƒì„±"""
        # í…Œì´ë¸” ë©”íƒ€ë°ì´í„°
        self.metadata_collection = self.client.get_or_create_collection(
            name="table_metadata",
            metadata={"description": "í…Œì´ë¸” ë° ì¹¼ëŸ¼ ë©”íƒ€ë°ì´í„°"}
        )

        # DDL ìŠ¤í‚¤ë§ˆ
        self.ddl_collection = self.client.get_or_create_collection(
            name="table_ddl",
            metadata={"description": "í…Œì´ë¸” DDL"}
        )

        # Few-shot ì˜ˆì œ
        self.fewshot_collection = self.client.get_or_create_collection(
            name="sql_examples",
            metadata={"description": "SQL Few-shot ì˜ˆì œ"}
        )

        # ì§€ì‹ ë² ì´ìŠ¤
        self.knowledge_collection = self.client.get_or_create_collection(
            name="knowledge_base",
            metadata={"description": "ì‚¬ë‚´ ë¬¸ì„œ"}
        )

    def add_documents(self, collection_name, documents, metadatas, ids):
        """ë¬¸ì„œ ì¶”ê°€"""
        collection = getattr(self, f"{collection_name}_collection")

        # ì„ë² ë”© ìƒì„±
        embeddings = self.embed_model.encode(documents).tolist()

        # ì¶”ê°€
        collection.add(
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )

    def search(self, collection_name, query, top_k=5, where=None):
        """ê²€ìƒ‰"""
        collection = getattr(self, f"{collection_name}_collection")

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embed_model.encode(query).tolist()

        # ê²€ìƒ‰
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            where=where
        )

        return results

    def persist(self):
        """ì˜êµ¬ ì €ì¥"""
        self.client.persist()
```

### 1.2 BM25 ê²€ìƒ‰ ì—”ì§„

**src/rag/bm25_search.py**:
```python
from rank_bm25 import BM25Okapi
import pickle
import numpy as np

class BM25Search:
    def __init__(self, corpus=None, index_path=None):
        if index_path and os.path.exists(index_path):
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
            with open(index_path, 'rb') as f:
                self.bm25 = pickle.load(f)
        elif corpus:
            # ìƒˆ ì¸ë±ìŠ¤ êµ¬ì¶•
            tokenized_corpus = [doc.split() for doc in corpus]
            self.bm25 = BM25Okapi(tokenized_corpus)
        else:
            raise ValueError("corpus ë˜ëŠ” index_path í•„ìš”")

        self.corpus = corpus

    def search(self, query, top_k=5):
        """ê²€ìƒ‰"""
        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)

        # ìƒìœ„ Kê°œ
        top_indices = np.argsort(scores)[-top_k:][::-1]
        results = [
            {
                "document": self.corpus[i],
                "score": scores[i],
                "index": i
            }
            for i in top_indices
        ]

        return results

    def save(self, path):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        with open(path, 'wb') as f:
            pickle.dump(self.bm25, f)
```

### 1.3 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

**src/rag/hybrid_search.py**:
```python
from .vector_store import VectorStore
from .bm25_search import BM25Search
import numpy as np

class HybridSearch:
    def __init__(self, vector_store, bm25_search):
        self.vector_store = vector_store
        self.bm25_search = bm25_search

    def search(self, query, collection_name, top_k=5, alpha=0.5):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        alpha: BM25 ê°€ì¤‘ì¹˜ (0~1)
        - 1.0: BM25ë§Œ
        - 0.0: ë²¡í„°ë§Œ
        - 0.5: 50% BM25 + 50% ë²¡í„° (ì¶”ì²œ)
        """
        # BM25 ê²€ìƒ‰
        bm25_results = self.bm25_search.search(query, top_k=top_k*2)
        bm25_scores = np.array([r['score'] for r in bm25_results])
        bm25_normalized = bm25_scores / (np.max(bm25_scores) + 1e-10)

        # ë²¡í„° ê²€ìƒ‰
        vector_results = self.vector_store.search(
            collection_name, query, top_k=top_k*2
        )
        vector_scores = np.array(vector_results['distances'][0])
        vector_normalized = 1 - vector_scores  # ê±°ë¦¬ â†’ ìœ ì‚¬ë„

        # ì ìˆ˜ í†µí•©
        combined_scores = (
            alpha * bm25_normalized +
            (1 - alpha) * vector_normalized
        )

        # ìƒìœ„ Kê°œ
        top_indices = np.argsort(combined_scores)[-top_k:][::-1]

        return [
            {
                "document": bm25_results[i]['document'],
                "score": combined_scores[i],
                "bm25_score": bm25_scores[i],
                "vector_score": vector_scores[i]
            }
            for i in top_indices
        ]
```

---

## ğŸ“‹ Phase 2: Text-to-SQL êµ¬í˜„ (1ì£¼)

### 2.1 ë°ì´í„°ë² ì´ìŠ¤ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘

**src/chains/metadata_collector.py**:
```python
from sqlalchemy import create_engine, inspect
import sqlite3
import json

class MetadataCollector:
    def __init__(self, db_url):
        self.engine = create_engine(db_url)
        self.inspector = inspect(self.engine)

    def collect_all_metadata(self):
        """ëª¨ë“  í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        metadata = []

        for table_name in self.inspector.get_table_names():
            table_meta = self.collect_table_metadata(table_name)
            metadata.append(table_meta)

        return metadata

    def collect_table_metadata(self, table_name):
        """ë‹¨ì¼ í…Œì´ë¸” ë©”íƒ€ë°ì´í„°"""
        columns = []

        for column in self.inspector.get_columns(table_name):
            columns.append({
                "name": column['name'],
                "type": str(column['type']),
                "nullable": column['nullable'],
                "default": column.get('default')
            })

        return {
            "table_name": table_name,
            "columns": columns,
            "primary_keys": self.inspector.get_pk_constraint(table_name)['constrained_columns'],
            "foreign_keys": [
                {
                    "columns": fk['constrained_columns'],
                    "referred_table": fk['referred_table'],
                    "referred_columns": fk['referred_columns']
                }
                for fk in self.inspector.get_foreign_keys(table_name)
            ]
        }

    def save_to_sqlite(self, metadata, db_path):
        """SQLiteì— ì €ì¥"""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # í…Œì´ë¸” ìƒì„±
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS table_metadata (
            table_name TEXT PRIMARY KEY,
            description TEXT,
            purpose TEXT,
            columns_json TEXT,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        ''')

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        for table in metadata:
            cursor.execute('''
            INSERT OR REPLACE INTO table_metadata
            (table_name, columns_json)
            VALUES (?, ?)
            ''', (table['table_name'], json.dumps(table)))

        conn.commit()
        conn.close()
```

### 2.2 Text-to-SQL ì²´ì¸

**src/chains/text_to_sql.py**:
```python
from langchain.chains import create_sql_query_chain
from langchain.sql_database import SQLDatabase
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from sqlalchemy import create_engine, text

class TextToSQLChain:
    def __init__(self, db_url, openai_api_key):
        self.engine = create_engine(db_url)
        self.db = SQLDatabase(self.engine)

        # LLM
        self.llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0,
            openai_api_key=openai_api_key
        )

        # í”„ë¡¬í”„íŠ¸
        self.prompt = self._create_prompt()

        # ì²´ì¸
        self.chain = create_sql_query_chain(
            self.llm,
            self.db,
            self.prompt
        )

    def _create_prompt(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        template = """
ë‹¹ì‹ ì€ PostgreSQL ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìì—°ì–´ ì§ˆë¬¸ì„ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì„¸ìš”.

ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ:
{schema}

Few-shot ì˜ˆì œ:
{few_shot_examples}

ì§ˆë¬¸: {question}

ê·œì¹™:
1. SELECT ë¬¸ë§Œ ìƒì„± (INSERT, UPDATE, DELETE ê¸ˆì§€)
2. ì‹¤í–‰ ê°€ëŠ¥í•œ ì™„ì „í•œ SQLë§Œ ë°˜í™˜
3. ì„¤ëª…ì´ë‚˜ ì£¼ì„ ì œì™¸
4. PostgreSQL ë¬¸ë²• ì‚¬ìš©

SQL ì¿¼ë¦¬:
"""
        return PromptTemplate(
            template=template,
            input_variables=["schema", "few_shot_examples", "question"]
        )

    def generate_sql(self, question, few_shot_examples=""):
        """SQL ìƒì„±"""
        # ê´€ë ¨ í…Œì´ë¸” ì„ ë³„ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
        relevant_tables = self._find_relevant_tables(question)

        # DDL ì¶•ì†Œ
        schema = self.db.get_table_info(relevant_tables)

        # í”„ë¡¬í”„íŠ¸ ì…ë ¥
        result = self.chain.invoke({
            "schema": schema,
            "few_shot_examples": few_shot_examples,
            "question": question
        })

        return result

    def _find_relevant_tables(self, question):
        """ê´€ë ¨ í…Œì´ë¸” ì°¾ê¸° (ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­)"""
        all_tables = self.db.get_usable_table_names()

        # TODO: ë²¡í„°/BM25 ê²€ìƒ‰ìœ¼ë¡œ ê°œì„ 
        # í˜„ì¬ëŠ” ëª¨ë“  í…Œì´ë¸” ë°˜í™˜ (í”„ë¡œí† íƒ€ì…)
        return all_tables[:5]  # ìµœëŒ€ 5ê°œ

    def validate_sql(self, sql):
        """SQL ê²€ì¦ (EXPLAIN)"""
        try:
            explain_query = f"EXPLAIN {sql}"
            with self.engine.connect() as conn:
                conn.execute(text(explain_query))
            return True, "Valid SQL"
        except Exception as e:
            return False, str(e)

    def execute_sql(self, sql, limit=100):
        """SQL ì‹¤í–‰ (READ-ONLY)"""
        # SELECT í™•ì¸
        if not sql.strip().upper().startswith("SELECT"):
            return None, "Only SELECT queries allowed"

        # LIMIT ì¶”ê°€
        if "LIMIT" not in sql.upper():
            sql = f"{sql} LIMIT {limit}"

        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(sql))
                rows = result.fetchall()
                columns = result.keys()

            return {
                "columns": list(columns),
                "rows": [list(row) for row in rows],
                "row_count": len(rows)
            }, None

        except Exception as e:
            return None, str(e)
```

---

## ğŸ“‹ Phase 3: Memory Management (3ì¼)

**src/agents/memory.py**:
```python
import sqlite3
import json
from datetime import datetime

class ConversationMemory:
    def __init__(self, db_path="./sqlite_data/conversations.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """DB ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
        CREATE TABLE IF NOT EXISTS conversations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            role TEXT NOT NULL,
            content TEXT NOT NULL,
            metadata TEXT
        )
        ''')

        cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_session_timestamp
        ON conversations(session_id, timestamp DESC)
        ''')

        conn.commit()
        conn.close()

    def add_message(self, session_id, role, content, metadata=None):
        """ë©”ì‹œì§€ ì¶”ê°€"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
        INSERT INTO conversations (session_id, role, content, metadata)
        VALUES (?, ?, ?, ?)
        ''', (session_id, role, content, json.dumps(metadata) if metadata else None))

        conn.commit()
        conn.close()

    def get_recent_messages(self, session_id, limit=5):
        """ìµœê·¼ ë©”ì‹œì§€ ì¡°íšŒ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
        SELECT role, content, timestamp
        FROM conversations
        WHERE session_id = ?
        ORDER BY timestamp DESC
        LIMIT ?
        ''', (session_id, limit))

        messages = []
        for row in cursor.fetchall():
            messages.append({
                "role": row[0],
                "content": row[1],
                "timestamp": row[2]
            })

        conn.close()

        # ì‹œê°„ìˆœ ì •ë ¬
        return list(reversed(messages))

    def reconstruct_question(self, session_id, current_question, llm):
        """ë§¥ë½ì„ ë°˜ì˜í•œ ì§ˆë¬¸ ì¬êµ¬ì„±"""
        recent = self.get_recent_messages(session_id, limit=3)

        if not recent:
            return current_question

        # í”„ë¡¬í”„íŠ¸
        context = "\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in recent
        ])

        prompt = f"""
ì´ì „ ëŒ€í™”:
{context}

í˜„ì¬ ì§ˆë¬¸: {current_question}

ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì„ ëª…í™•í•˜ê²Œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
ì¬êµ¬ì„±ëœ ì§ˆë¬¸ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""

        response = llm.invoke(prompt)
        return response.content
```

---

## ğŸ“‹ Phase 4: Streamlit UI (3ì¼)

**src/ui/app.py**:
```python
import streamlit as st
from src.chains.text_to_sql import TextToSQLChain
from src.agents.memory import ConversationMemory
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv

load_dotenv()

# ì„¤ì •
st.set_page_config(
    page_title="AI ë°ì´í„° ë¶„ì„ê°€",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ì´ˆê¸°í™”
@st.cache_resource
def init_components():
    sql_chain = TextToSQLChain(
        db_url=os.getenv("DATABASE_URL"),
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )

    memory = ConversationMemory()

    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )

    return sql_chain, memory, llm

sql_chain, memory, llm = init_components()

# UI
st.title("ğŸ¤– AI ë°ì´í„° ë¶„ì„ê°€")
st.caption("ìì—°ì–´ë¡œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì§ˆë¬¸í•˜ì„¸ìš”")

# ì„¸ì…˜ ID
if "session_id" not in st.session_state:
    import uuid
    st.session_state.session_id = str(uuid.uuid4())

# ëŒ€í™” ì´ë ¥
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

        # SQL ì¿¼ë¦¬ í‘œì‹œ
        if "sql" in message:
            with st.expander("ìƒì„±ëœ SQL"):
                st.code(message["sql"], language="sql")

        # ê²°ê³¼ í‘œì‹œ
        if "result" in message:
            with st.expander("ì‹¤í–‰ ê²°ê³¼"):
                st.dataframe(message["result"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€
    with st.chat_message("user"):
        st.markdown(prompt)

    st.session_state.messages.append({"role": "user", "content": prompt})
    memory.add_message(st.session_state.session_id, "user", prompt)

    # AI ì‘ë‹µ
    with st.chat_message("assistant"):
        with st.spinner("SQL ìƒì„± ì¤‘..."):
            # ì§ˆë¬¸ ì¬êµ¬ì„±
            reconstructed = memory.reconstruct_question(
                st.session_state.session_id,
                prompt,
                llm
            )

            # SQL ìƒì„±
            sql = sql_chain.generate_sql(reconstructed)

            # SQL ê²€ì¦
            valid, msg = sql_chain.validate_sql(sql)

            if not valid:
                st.error(f"SQL ì˜¤ë¥˜: {msg}")
                response = f"SQL ìƒì„± ì‹¤íŒ¨: {msg}"
            else:
                # SQL ì‹¤í–‰
                result, error = sql_chain.execute_sql(sql)

                if error:
                    st.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {error}")
                    response = f"ì‹¤í–‰ ì‹¤íŒ¨: {error}"
                else:
                    # ê²°ê³¼ í‘œì‹œ
                    st.success(f"âœ… {result['row_count']}ê°œ í–‰ ì¡°íšŒ")

                    with st.expander("ìƒì„±ëœ SQL", expanded=True):
                        st.code(sql, language="sql")

                    with st.expander("ì‹¤í–‰ ê²°ê³¼", expanded=True):
                        import pandas as pd
                        df = pd.DataFrame(result['rows'], columns=result['columns'])
                        st.dataframe(df)

                    response = f"ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ: {result['row_count']}ê°œ í–‰"

                    # ë©”ì‹œì§€ì— ì¶”ê°€ ì •ë³´ ì €ì¥
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": response,
                        "sql": sql,
                        "result": df
                    })

                    memory.add_message(
                        st.session_state.session_id,
                        "assistant",
                        response,
                        {"sql": sql, "row_count": result['row_count']}
                    )

            st.markdown(response)
```

**ì‹¤í–‰**:
```bash
streamlit run src/ui/app.py
```

---

## ğŸ“‹ Phase 5: í†µí•© ë° í…ŒìŠ¤íŠ¸ (3ì¼)

### í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

**tests/test_text_to_sql.py**:
```python
import pytest
from src.chains.text_to_sql import TextToSQLChain
import os

@pytest.fixture
def sql_chain():
    return TextToSQLChain(
        db_url=os.getenv("DATABASE_URL"),
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )

def test_simple_query(sql_chain):
    """ê°„ë‹¨í•œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
    question = "ì‚¬ìš©ì ìˆ˜ëŠ”?"
    sql = sql_chain.generate_sql(question)

    assert "SELECT" in sql.upper()
    assert "COUNT" in sql.upper()

    # ê²€ì¦
    valid, msg = sql_chain.validate_sql(sql)
    assert valid, f"SQL ê²€ì¦ ì‹¤íŒ¨: {msg}"

def test_complex_query(sql_chain):
    """ë³µì¡í•œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
    question = "ì§€ë‚œë‹¬ ì‹ ê·œ ê°€ì…ì ìˆ˜ëŠ”?"
    sql = sql_chain.generate_sql(question)

    assert "SELECT" in sql.upper()
    assert "WHERE" in sql.upper()

    valid, _ = sql_chain.validate_sql(sql)
    assert valid
```

**ì‹¤í–‰**:
```bash
pytest tests/
```

---

## ğŸ“‹ Phase 6: ë°°í¬ (2ì¼)

### Docker Compose ë°°í¬

**docker-compose.yml**:
```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: ai_data_tool
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  app:
    build: .
    depends_on:
      - postgres
    environment:
      DATABASE_URL: postgresql://user:password@postgres:5432/ai_data_tool
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    volumes:
      - ./chromadb_data:/app/chromadb_data
      - ./sqlite_data:/app/sqlite_data
    ports:
      - "8501:8501"
    command: streamlit run src/ui/app.py

volumes:
  postgres_data:
```

**Dockerfile**:
```dockerfile
FROM python:3.10-slim

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì†ŒìŠ¤ ë³µì‚¬
COPY . .

# í¬íŠ¸
EXPOSE 8501

# ì‹¤í–‰
CMD ["streamlit", "run", "src/ui/app.py"]
```

**ì‹¤í–‰**:
```bash
docker-compose up -d
```

---

## âœ… êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 0: í™˜ê²½ ì„¤ì •
- [ ] Python 3.10+ ì„¤ì¹˜
- [ ] PostgreSQL ì„¤ì¹˜ ë° DB ìƒì„±
- [ ] ê°€ìƒí™˜ê²½ ìƒì„± ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜
- [ ] .env íŒŒì¼ ì„¤ì •
- [ ] ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±

### Phase 1: RAG ì‹œìŠ¤í…œ
- [ ] ChromaDB ì´ˆê¸°í™”
- [ ] sentence-transformers ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- [ ] BM25 ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„
- [ ] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í†µí•©
- [ ] ë¬¸ì„œ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸

### Phase 2: Text-to-SQL
- [ ] ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ê¸° êµ¬í˜„
- [ ] Few-shot ì˜ˆì œ ìˆ˜ì§‘
- [ ] SQL ìƒì„± ì²´ì¸ êµ¬í˜„
- [ ] SQL ê²€ì¦ ë¡œì§
- [ ] SQL ì‹¤í–‰ ì—”ì§„ (READ-ONLY)

### Phase 3: Memory Management
- [ ] SQLite ëŒ€í™” ì´ë ¥ DB
- [ ] ë©”ì‹œì§€ ì €ì¥/ì¡°íšŒ
- [ ] ì§ˆë¬¸ ì¬êµ¬ì„± ë¡œì§

### Phase 4: Streamlit UI
- [ ] ê¸°ë³¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
- [ ] SQL í‘œì‹œ ë° ê²°ê³¼ í…Œì´ë¸”
- [ ] ì„¸ì…˜ ê´€ë¦¬
- [ ] í”¼ë“œë°± ìˆ˜ì§‘

### Phase 5: í†µí•© & í…ŒìŠ¤íŠ¸
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] í†µí•© í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- [ ] ì‚¬ìš©ì í…ŒìŠ¤íŠ¸

### Phase 6: ë°°í¬
- [ ] Docker ì´ë¯¸ì§€ ë¹Œë“œ
- [ ] docker-compose ì„¤ì •
- [ ] ë¡œì»¬ ë°°í¬ í…ŒìŠ¤íŠ¸
- [ ] í”„ë¡œë•ì…˜ ë°°í¬ (ì„ íƒ)

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

êµ¬í˜„ ì™„ë£Œ í›„:
1. **ì„±ëŠ¥ ìµœì í™”** (05-ì„±ëŠ¥-ìµœì í™”-ì˜¤í”ˆì†ŒìŠ¤.md ì°¸ì¡°)
2. **ì¶”ê°€ ê¸°ëŠ¥ êµ¬í˜„**:
   - Data Discovery
   - Knowledge Discovery
   - Support Automation
3. **ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…** ê°•í™”
4. **ì‚¬ìš©ì í”¼ë“œë°±** ìˆ˜ì§‘ ë° ê°œì„ 

---

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ **ì™„ì „ ë¬´ë£Œ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ AI ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ**ì„ 4-6ì£¼ ì•ˆì— êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
