# AI ë°ì´í„° ë¶„ì„ê°€ 'ë¬¼ì–´ë³´ìƒˆ' - ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ (ì˜¤í”ˆì†ŒìŠ¤)

## ğŸ“Š ê°œìš”

ë¬¼ì–´ë³´ìƒˆ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ ë¬´ë£Œ ì†”ë£¨ì…˜ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë¹„ìš©ì„ ìµœì†Œí™”í•˜ë©´ì„œë„ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤.

**ìµœì í™” ëª©í‘œ**:
- **ì‘ë‹µ ì‹œê°„**: Text-to-SQL 30ì´ˆ~1ë¶„, Knowledge Discovery ì‹¤ì‹œê°„
- **ë¹„ìš©**: LLM API ë¹„ìš© ìµœì†Œí™” (ì›” $50 ì´í•˜ ëª©í‘œ)
- **ì •í™•ë„**: ì¿¼ë¦¬ ìƒì„± ì •í™•ë„ 85% ì´ìƒ
- **ì²˜ë¦¬ëŸ‰**: ë™ì‹œ ì‚¬ìš©ì 50ëª… ì´ìƒ ì§€ì›

---

## ğŸš€ 1. LLM API ë¹„ìš© ìµœì í™”

### 1.1 í”„ë¡¬í”„íŠ¸ ì••ì¶• ì „ëµ

**ë¬¸ì œ**: GPT-4o API ë¹„ìš©ì´ í† í° ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ì¦ê°€

**í•´ê²°ì±…**: íš¨ìœ¨ì ì¸ í”„ë¡¬í”„íŠ¸ ì„¤ê³„

```python
# âŒ ë¹„íš¨ìœ¨ì  (í† í° ë‚­ë¹„)
prompt = f"""
You are a helpful SQL assistant.
Given the following database schema with all its details:
{full_schema}  # ì „ì²´ ìŠ¤í‚¤ë§ˆ (ìˆ˜ì²œ í† í°)

And here are many examples of how to write queries:
{all_examples}  # ëª¨ë“  ì˜ˆì œ (ìˆ˜ì²œ í† í°)

Now please write a SQL query for: {question}
"""

# âœ… íš¨ìœ¨ì  (í† í° ì ˆì•½)
# 1. ê´€ë ¨ í…Œì´ë¸”ë§Œ ì„ ë³„
relevant_tables = self.select_relevant_tables(question, top_k=3)
schema = self.db.get_table_info(relevant_tables)

# 2. ê´€ë ¨ ì˜ˆì œë§Œ ê²€ìƒ‰
relevant_examples = self.retrieve_examples(question, top_k=2)

# 3. ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
prompt = f"""Schema: {schema}
Examples: {relevant_examples}
Question: {question}
SQL:"""
```

**íš¨ê³¼**: í† í° ì‚¬ìš©ëŸ‰ 60-70% ê°ì†Œ

### 1.2 ì‘ë‹µ ìºì‹± ì‹œìŠ¤í…œ

**ê°œë…**: ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ì§ˆë¬¸ì˜ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©

```python
import hashlib
import json
from datetime import datetime, timedelta

class ResponseCache:
    def __init__(self, db_path="cache.db", ttl_hours=24):
        self.conn = sqlite3.connect(db_path)
        self.ttl_hours = ttl_hours
        self._create_table()

    def _create_table(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS response_cache (
                cache_key TEXT PRIMARY KEY,
                question TEXT,
                response TEXT,
                created_at TIMESTAMP,
                hit_count INTEGER DEFAULT 1
            )
        """)

    def get_cache_key(self, question, context=""):
        """ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{question}|{context}"
        return hashlib.sha256(content.encode()).hexdigest()

    def get(self, question, context=""):
        """ìºì‹œì—ì„œ ì‘ë‹µ ì¡°íšŒ"""
        cache_key = self.get_cache_key(question, context)
        cutoff_time = datetime.now() - timedelta(hours=self.ttl_hours)

        cursor = self.conn.execute("""
            SELECT response, hit_count
            FROM response_cache
            WHERE cache_key = ? AND created_at > ?
        """, (cache_key, cutoff_time))

        result = cursor.fetchone()
        if result:
            # íˆíŠ¸ ì¹´ìš´íŠ¸ ì¦ê°€
            self.conn.execute("""
                UPDATE response_cache
                SET hit_count = hit_count + 1
                WHERE cache_key = ?
            """, (cache_key,))
            self.conn.commit()
            return result[0]
        return None

    def set(self, question, response, context=""):
        """ì‘ë‹µì„ ìºì‹œì— ì €ì¥"""
        cache_key = self.get_cache_key(question, context)
        self.conn.execute("""
            INSERT OR REPLACE INTO response_cache
            (cache_key, question, response, created_at)
            VALUES (?, ?, ?, ?)
        """, (cache_key, question, response, datetime.now()))
        self.conn.commit()

    def get_stats(self):
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        cursor = self.conn.execute("""
            SELECT
                COUNT(*) as total_entries,
                SUM(hit_count) as total_hits,
                AVG(hit_count) as avg_hits_per_entry
            FROM response_cache
        """)
        return cursor.fetchone()

# ì‚¬ìš© ì˜ˆì œ
cache = ResponseCache(ttl_hours=24)

def generate_sql_with_cache(question):
    # 1. ìºì‹œ í™•ì¸
    cached_response = cache.get(question)
    if cached_response:
        print("âœ… ìºì‹œ íˆíŠ¸! LLM API í˜¸ì¶œ ìƒëµ")
        return cached_response

    # 2. ìºì‹œ ë¯¸ìŠ¤ â†’ LLM API í˜¸ì¶œ
    print("â³ LLM API í˜¸ì¶œ ì¤‘...")
    response = llm_chain.invoke({"question": question})

    # 3. ê²°ê³¼ ìºì‹±
    cache.set(question, response)

    return response
```

**íš¨ê³¼**:
- ë°˜ë³µ ì§ˆë¬¸ 70% ê°ì†Œ ì‹œ API ë¹„ìš© 70% ì ˆê°
- ì‘ë‹µ ì‹œê°„ 30ì´ˆ â†’ 0.1ì´ˆ

### 1.3 ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

**ê°œë…**: ì‘ë‹µì„ í•œ ë²ˆì— ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ì ì§„ì ìœ¼ë¡œ í‘œì‹œ

```python
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# âœ… ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
llm = AzureChatOpenAI(
    temperature=0,
    streaming=True,  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
    callbacks=[StreamingStdOutCallbackHandler()]
)

# Streamlit UIì—ì„œ ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ
def display_streaming_response(chain, question):
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        for chunk in chain.stream({"question": question}):
            full_response += chunk
            message_placeholder.markdown(full_response + "â–Œ")

        message_placeholder.markdown(full_response)

    return full_response
```

**íš¨ê³¼**:
- ì‚¬ìš©ì ì²´ê° ëŒ€ê¸° ì‹œê°„ 50% ê°ì†Œ
- ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ

### 1.4 í”„ë¡¬í”„íŠ¸ ì¬ì‚¬ìš© ìµœì í™”

**ê°œë…**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìºì‹±í•˜ì—¬ ë°˜ë³µ ì „ì†¡ ë°©ì§€

```python
class PromptOptimizer:
    def __init__(self):
        # ê³ ì • ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í•œ ë²ˆë§Œ ì „ì†¡)
        self.system_prompt = """You are an expert SQL assistant.
Follow these rules:
1. Generate syntactically correct PostgreSQL queries
2. Use table and column names exactly as provided
3. Include relevant JOINs when needed
4. Return only the SQL query without explanation"""

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ë³€ìˆ˜ë§Œ êµì²´)
        self.query_template = """Schema: {schema}
Examples: {examples}
Question: {question}
SQL:"""

    def build_prompt(self, schema, examples, question):
        """ìµœì†Œ í† í°ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        user_prompt = self.query_template.format(
            schema=schema,
            examples=examples,
            question=question
        )

        return [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]

optimizer = PromptOptimizer()

# ë§¤ ìš”ì²­ë§ˆë‹¤ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¬ì‚¬ìš©
messages = optimizer.build_prompt(schema, examples, question)
response = llm.invoke(messages)
```

**íš¨ê³¼**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í† í° ë¹„ìš© 100% ì¬ì‚¬ìš©

### 1.5 ë¹„ìš© ëª¨ë‹ˆí„°ë§

```python
import tiktoken

class CostTracker:
    def __init__(self):
        self.encoding = tiktoken.encoding_for_model("gpt-4o")
        self.costs = {
            "gpt-4o": {"input": 0.005, "output": 0.015}  # per 1K tokens
        }
        self.total_input_tokens = 0
        self.total_output_tokens = 0

    def count_tokens(self, text):
        return len(self.encoding.encode(text))

    def track_request(self, prompt, response):
        input_tokens = self.count_tokens(prompt)
        output_tokens = self.count_tokens(response)

        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens

        cost = (
            (input_tokens / 1000) * self.costs["gpt-4o"]["input"] +
            (output_tokens / 1000) * self.costs["gpt-4o"]["output"]
        )

        return {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost
        }

    def get_total_cost(self):
        total_cost = (
            (self.total_input_tokens / 1000) * self.costs["gpt-4o"]["input"] +
            (self.total_output_tokens / 1000) * self.costs["gpt-4o"]["output"]
        )

        return {
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "total_cost": round(total_cost, 4)
        }

# ì‚¬ìš© ì˜ˆì œ
tracker = CostTracker()

def generate_with_tracking(prompt):
    response = llm.invoke(prompt)
    stats = tracker.track_request(prompt, response)

    print(f"ğŸ’° ë¹„ìš©: ${stats['cost']:.4f}")
    print(f"ğŸ“Š ì…ë ¥ í† í°: {stats['input_tokens']}, ì¶œë ¥ í† í°: {stats['output_tokens']}")

    return response

# ì¼ì¼ ë¹„ìš© í™•ì¸
total_stats = tracker.get_total_cost()
print(f"ğŸ“ˆ ì¼ì¼ ì´ ë¹„ìš©: ${total_stats['total_cost']:.2f}")
```

**íš¨ê³¼**: ì‹¤ì‹œê°„ ë¹„ìš© ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì˜ˆì‚° ì´ˆê³¼ ë°©ì§€

---

## âš¡ 2. ì‘ë‹µ ì‹œê°„ ìµœì í™”

### 2.1 BM25 ì‚¬ì „ ê³„ì‚°

**ë¬¸ì œ**: BM25 í† í¬ë‚˜ì´ì§• ë° ì ìˆ˜ ê³„ì‚°ì— ì‹œê°„ ì†Œìš”

**í•´ê²°ì±…**: ì‚¬ì „ ê³„ì‚° ë° ì¸ë±ì‹±

```python
import pickle
from rank_bm25 import BM25Okapi
from typing import List, Dict

class PrecomputedBM25:
    def __init__(self, corpus_path="corpus.pkl", index_path="bm25_index.pkl"):
        self.corpus_path = corpus_path
        self.index_path = index_path
        self.bm25 = None
        self.corpus = None
        self.metadata = None

    def build_index(self, documents: List[Dict]):
        """BM25 ì¸ë±ìŠ¤ ì‚¬ì „ êµ¬ì¶•"""
        print("ğŸ”¨ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

        # 1. ë¬¸ì„œ í† í¬ë‚˜ì´ì§•
        self.corpus = [self.tokenize(doc["content"]) for doc in documents]
        self.metadata = [
            {"id": doc["id"], "title": doc.get("title", "")}
            for doc in documents
        ]

        # 2. BM25 ì¸ë±ìŠ¤ ìƒì„±
        self.bm25 = BM25Okapi(self.corpus)

        # 3. ë””ìŠ¤í¬ì— ì €ì¥
        with open(self.corpus_path, "wb") as f:
            pickle.dump((self.corpus, self.metadata), f)

        with open(self.index_path, "wb") as f:
            pickle.dump(self.bm25, f)

        print(f"âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")

    def load_index(self):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        with open(self.corpus_path, "rb") as f:
            self.corpus, self.metadata = pickle.load(f)

        with open(self.index_path, "rb") as f:
            self.bm25 = pickle.load(f)

        print(f"âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.corpus)}ê°œ ë¬¸ì„œ")

    def tokenize(self, text):
        """í•œê¸€ í† í¬ë‚˜ì´ì§•"""
        # ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜ (ì‹¤ì œë¡œëŠ” KoNLPy ë“± ì‚¬ìš© ê°€ëŠ¥)
        return text.lower().split()

    def search(self, query: str, top_k: int = 5):
        """ë¹ ë¥¸ ê²€ìƒ‰"""
        if self.bm25 is None:
            self.load_index()

        tokenized_query = self.tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)

        # ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
        top_indices = sorted(
            range(len(scores)),
            key=lambda i: scores[i],
            reverse=True
        )[:top_k]

        results = []
        for idx in top_indices:
            results.append({
                "score": float(scores[idx]),
                "metadata": self.metadata[idx]
            })

        return results

# ì‚¬ìš© ì˜ˆì œ
# 1. ìµœì´ˆ í•œ ë²ˆë§Œ ì¸ë±ìŠ¤ êµ¬ì¶•
bm25_index = PrecomputedBM25()
documents = [...]  # ë¬¸ì„œ ë¡œë“œ
bm25_index.build_index(documents)

# 2. ì´í›„ì—ëŠ” ë¡œë“œë§Œ (0.1ì´ˆ ì´ë‚´)
bm25_index = PrecomputedBM25()
bm25_index.load_index()

# 3. ë¹ ë¥¸ ê²€ìƒ‰ (0.01ì´ˆ)
results = bm25_index.search("ë°°ë¯¼ ì‹ ê·œ ê°€ì…ì", top_k=5)
```

**íš¨ê³¼**:
- ê²€ìƒ‰ ì‹œê°„ 5ì´ˆ â†’ 0.01ì´ˆ (500ë°° ë¹ ë¦„)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ (trade-off)

### 2.2 ë³‘ë ¬ ì²˜ë¦¬

**ê°œë…**: ë…ë¦½ì ì¸ ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

class ParallelProcessor:
    def __init__(self, max_workers=5):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

    def parallel_search(self, query, top_k=5):
        """Vector + BM25 ê²€ìƒ‰ ë³‘ë ¬ ì‹¤í–‰"""

        # ìˆœì°¨ ì‹¤í–‰ (ëŠë¦¼)
        # vector_results = vector_store.search(query, top_k)  # 0.5ì´ˆ
        # bm25_results = bm25_search.search(query, top_k)     # 0.01ì´ˆ
        # Total: 0.51ì´ˆ

        # ë³‘ë ¬ ì‹¤í–‰ (ë¹ ë¦„)
        futures = {
            self.executor.submit(vector_store.search, query, top_k): "vector",
            self.executor.submit(bm25_search.search, query, top_k): "bm25"
        }

        results = {}
        for future in as_completed(futures):
            search_type = futures[future]
            results[search_type] = future.result()

        # Total: max(0.5, 0.01) = 0.5ì´ˆ
        return results

    def parallel_table_analysis(self, question):
        """í…Œì´ë¸” ì„ ë³„ê³¼ ì˜ˆì œ ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ"""

        futures = {
            self.executor.submit(self.select_tables, question): "tables",
            self.executor.submit(self.retrieve_examples, question): "examples"
        }

        results = {}
        for future in as_completed(futures):
            task_type = futures[future]
            results[task_type] = future.result()

        return results

# ì‚¬ìš© ì˜ˆì œ
processor = ParallelProcessor(max_workers=5)

# ë³‘ë ¬ ê²€ìƒ‰
start = time.time()
search_results = processor.parallel_search("ë°°ë¯¼ ì£¼ë¬¸ í˜„í™©")
print(f"âš¡ ê²€ìƒ‰ ì‹œê°„: {time.time() - start:.2f}ì´ˆ")

# ë³‘ë ¬ ì „ì²˜ë¦¬
prep_results = processor.parallel_table_analysis("ì§€ë‚œì£¼ ì£¼ë¬¸ëŸ‰ì€?")
```

**íš¨ê³¼**:
- ì‘ë‹µ ì‹œê°„ 30-40% ê°ì†Œ
- CPU í™œìš©ë¥  í–¥ìƒ

### 2.3 ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬

**ë¬¸ì œ**: ë¬¸ì„œë¥¼ í•˜ë‚˜ì”© ì„ë² ë”©í•˜ë©´ ëŠë¦¼

**í•´ê²°ì±…**: ë°°ì¹˜ ì²˜ë¦¬

```python
from sentence_transformers import SentenceTransformer
import numpy as np

class BatchEmbedder:
    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
        self.model = SentenceTransformer(model_name)
        self.model.to('cuda')  # GPU ì‚¬ìš© (ìˆëŠ” ê²½ìš°)

    def embed_documents(self, texts: List[str], batch_size=32):
        """ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±"""

        # âŒ ëŠë¦° ë°©ì‹ (í•˜ë‚˜ì”©)
        # embeddings = [self.model.encode(text) for text in texts]

        # âœ… ë¹ ë¥¸ ë°©ì‹ (ë°°ì¹˜)
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,
            normalize_embeddings=True
        )

        return embeddings

    def incremental_index(self, new_documents, existing_index):
        """ì¦ë¶„ ì¸ë±ì‹±"""
        # ìƒˆ ë¬¸ì„œë§Œ ì„ë² ë”©
        new_embeddings = self.embed_documents(
            [doc["content"] for doc in new_documents],
            batch_size=32
        )

        # ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€
        existing_index.add(new_embeddings)

        return existing_index

# ì‚¬ìš© ì˜ˆì œ
embedder = BatchEmbedder()

# 1000ê°œ ë¬¸ì„œ ì„ë² ë”©
documents = load_documents()  # 1000ê°œ
embeddings = embedder.embed_documents(
    [doc["content"] for doc in documents],
    batch_size=64
)

# ì†ë„ ë¹„êµ:
# - í•˜ë‚˜ì”©: 100ì´ˆ
# - ë°°ì¹˜ 32: 15ì´ˆ (6.6ë°° ë¹ ë¦„)
# - ë°°ì¹˜ 64: 12ì´ˆ (8.3ë°° ë¹ ë¦„)
```

**íš¨ê³¼**:
- ì„ë² ë”© ìƒì„± ì‹œê°„ 80% ê°ì†Œ
- GPU í™œìš© ì‹œ ì¶”ê°€ 10ë°° ì†ë„ í–¥ìƒ

### 2.4 ë©”ëª¨ë¦¬ ìµœì í™”

```python
import gc
import torch

class MemoryOptimizer:
    @staticmethod
    def optimize_sentence_transformer(model):
        """sentence-transformers ë©”ëª¨ë¦¬ ìµœì í™”"""

        # FP16 (ë°˜ì •ë°€ë„) ì‚¬ìš©
        if torch.cuda.is_available():
            model.half()  # FP32 â†’ FP16 (ë©”ëª¨ë¦¬ 50% ì ˆê°)

        return model

    @staticmethod
    def clear_cache():
        """ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    @staticmethod
    def optimize_chromadb_settings():
        """ChromaDB ë©”ëª¨ë¦¬ ì„¤ì •"""
        return {
            "chroma_db_impl": "duckdb+parquet",  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
            "anonymized_telemetry": False,
            "allow_reset": True,
            "is_persistent": True,
            # HNSW íŒŒë¼ë¯¸í„° ì¡°ì •
            "hnsw_space": "cosine",
            "hnsw_construction_ef": 100,  # ë‚®ì¶”ë©´ ë©”ëª¨ë¦¬â†“ ì •í™•ë„â†“
            "hnsw_search_ef": 100,
            "hnsw_M": 16  # ë‚®ì¶”ë©´ ë©”ëª¨ë¦¬â†“ ì„±ëŠ¥â†“
        }

# ì‚¬ìš© ì˜ˆì œ
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
model = MemoryOptimizer.optimize_sentence_transformer(model)

# ChromaDB ì„¤ì •
settings = MemoryOptimizer.optimize_chromadb_settings()
client = chromadb.Client(Settings(**settings))
```

**íš¨ê³¼**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 40-50% ê°ì†Œ

---

## ğŸ¯ 3. ì •í™•ë„ ê°œì„ 

### 3.1 ë©”íƒ€ë°ì´í„° í’ˆì§ˆ ê³ ë„í™”

**í•µì‹¬**: "Garbage in, Garbage out" - ë©”íƒ€ë°ì´í„° í’ˆì§ˆì´ ë‹µë³€ í’ˆì§ˆì„ ê²°ì •

```python
class MetadataEnhancer:
    def enhance_table_metadata(self, table_name, db_connection):
        """í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ê³ ë„í™”"""

        metadata = {
            "table_name": table_name,
            "description": "",
            "columns": [],
            "business_terms": [],
            "usage_examples": [],
            "data_quality": {}
        }

        # 1. ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ì •ë³´
        cursor = db_connection.execute(f"""
            SELECT column_name, data_type, is_nullable
            FROM information_schema.columns
            WHERE table_name = '{table_name}'
        """)

        for row in cursor.fetchall():
            metadata["columns"].append({
                "name": row[0],
                "type": row[1],
                "nullable": row[2],
                "description": self.get_column_description(table_name, row[0]),
                "sample_values": self.get_sample_values(table_name, row[0])
            })

        # 2. ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´ ë§¤í•‘
        metadata["business_terms"] = self.extract_business_terms(table_name)

        # 3. ì‚¬ìš© ì˜ˆì œ ì¶”ê°€
        metadata["usage_examples"] = self.generate_usage_examples(table_name)

        # 4. ë°ì´í„° í’ˆì§ˆ ì •ë³´
        metadata["data_quality"] = self.analyze_data_quality(table_name)

        return metadata

    def get_column_description(self, table_name, column_name):
        """ì¹¼ëŸ¼ ì„¤ëª… (ë°ì´í„° ë‹´ë‹¹ìì™€ í˜‘ì—…)"""
        descriptions = {
            "users.created_at": "ì‚¬ìš©ì ê³„ì • ìƒì„± ì‹œê° (KST ê¸°ì¤€)",
            "orders.order_date": "ì£¼ë¬¸ ì ‘ìˆ˜ ì‹œê° (UTC ì €ì¥, ì¡°íšŒ ì‹œ KST ë³€í™˜ í•„ìš”)",
            "users.status": "ì‚¬ìš©ì ìƒíƒœ (active/inactive/suspended)",
        }

        key = f"{table_name}.{column_name}"
        return descriptions.get(key, "")

    def get_sample_values(self, table_name, column_name):
        """ëŒ€í‘œ ê°’ ìƒ˜í”Œ"""
        # ENUM íƒ€ì…ì´ë‚˜ ì œí•œëœ ê°’ì˜ ê²½ìš° ì „ì²´ ëª©ë¡
        if column_name == "status":
            return ["active", "inactive", "suspended"]

        # ì¼ë°˜ ì¹¼ëŸ¼ì€ ìƒ˜í”Œ 3-5ê°œ
        cursor = db_connection.execute(f"""
            SELECT DISTINCT {column_name}
            FROM {table_name}
            LIMIT 5
        """)
        return [row[0] for row in cursor.fetchall()]

    def extract_business_terms(self, table_name):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´ ì¶”ì¶œ"""
        term_mappings = {
            "users": ["íšŒì›", "ê³ ê°", "ì‚¬ìš©ì", "ìœ ì €"],
            "orders": ["ì£¼ë¬¸", "êµ¬ë§¤", "ì˜¤ë”"],
            "products": ["ìƒí’ˆ", "ì œí’ˆ", "ë©”ë‰´", "ì•„ì´í…œ"]
        }

        return term_mappings.get(table_name, [])

    def analyze_data_quality(self, table_name):
        """ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
        cursor = db_connection.execute(f"""
            SELECT
                COUNT(*) as total_rows,
                COUNT(DISTINCT id) as unique_ids,
                SUM(CASE WHEN created_at IS NULL THEN 1 ELSE 0 END) as null_created_at
            FROM {table_name}
        """)

        row = cursor.fetchone()
        return {
            "total_rows": row[0],
            "unique_ids": row[1],
            "completeness": {
                "created_at": 1 - (row[2] / row[0]) if row[0] > 0 else 0
            }
        }

# ì‚¬ìš© ì˜ˆì œ
enhancer = MetadataEnhancer()
metadata = enhancer.enhance_table_metadata("users", db_connection)

# Vector DBì— ì €ì¥
vector_store.add_documents([
    {
        "content": f"""í…Œì´ë¸”: {metadata['table_name']}
ì„¤ëª…: {metadata['description']}
ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´: {', '.join(metadata['business_terms'])}
ì¹¼ëŸ¼: {', '.join([f"{col['name']} ({col['description']})" for col in metadata['columns']])}
ì‚¬ìš© ì˜ˆì œ: {metadata['usage_examples']}""",
        "metadata": metadata
    }
])
```

**íš¨ê³¼**: ì¿¼ë¦¬ ìƒì„± ì •í™•ë„ 60% â†’ 85%

### 3.2 Few-shot ì˜ˆì œ ìµœì í™”

**ì „ëµ**: ì§ˆ ë†’ì€ ì˜ˆì œë¥¼ ì§€ì†ì ìœ¼ë¡œ ì¶•ì 

```python
class FewShotManager:
    def __init__(self, db_path="fewshot_examples.db"):
        self.conn = sqlite3.connect(db_path)
        self._create_table()

    def _create_table(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS few_shot_examples (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                question TEXT NOT NULL,
                sql_query TEXT NOT NULL,
                category TEXT,
                difficulty TEXT,  -- easy/medium/hard
                success_rate REAL DEFAULT 0,
                usage_count INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        self.conn.commit()

    def add_example(self, question, sql_query, category="general", difficulty="medium"):
        """ìƒˆ ì˜ˆì œ ì¶”ê°€"""
        self.conn.execute("""
            INSERT INTO few_shot_examples (question, sql_query, category, difficulty)
            VALUES (?, ?, ?, ?)
        """, (question, sql_query, category, difficulty))
        self.conn.commit()

    def retrieve_examples(self, question, top_k=3, min_success_rate=0.7):
        """ìœ ì‚¬ ì˜ˆì œ ê²€ìƒ‰"""

        # 1. ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
        query_embedding = embed_model.encode(question)

        # 2. ëª¨ë“  ì˜ˆì œ ê°€ì ¸ì˜¤ê¸° (ì„±ê³µë¥  í•„í„°ë§)
        cursor = self.conn.execute("""
            SELECT id, question, sql_query, success_rate, usage_count
            FROM few_shot_examples
            WHERE success_rate >= ?
            ORDER BY usage_count DESC
            LIMIT 100
        """, (min_success_rate,))

        examples = cursor.fetchall()

        # 3. ìœ ì‚¬ë„ ê³„ì‚°
        example_embeddings = embed_model.encode([ex[1] for ex in examples])
        similarities = cosine_similarity([query_embedding], example_embeddings)[0]

        # 4. ìƒìœ„ kê°œ ì„ íƒ
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        selected_examples = []
        for idx in top_indices:
            ex = examples[idx]
            selected_examples.append({
                "question": ex[1],
                "sql_query": ex[2],
                "similarity": float(similarities[idx])
            })

            # ì‚¬ìš© íšŸìˆ˜ ì¦ê°€
            self.conn.execute("""
                UPDATE few_shot_examples
                SET usage_count = usage_count + 1
                WHERE id = ?
            """, (ex[0],))

        self.conn.commit()
        return selected_examples

    def update_success_rate(self, example_id, was_successful):
        """ì˜ˆì œ ì„±ê³µë¥  ì—…ë°ì´íŠ¸"""
        cursor = self.conn.execute("""
            SELECT success_rate, usage_count
            FROM few_shot_examples
            WHERE id = ?
        """, (example_id,))

        row = cursor.fetchone()
        if row:
            old_rate, count = row
            # ì´ë™ í‰ê· ìœ¼ë¡œ ì„±ê³µë¥  ì—…ë°ì´íŠ¸
            new_rate = (old_rate * count + (1 if was_successful else 0)) / (count + 1)

            self.conn.execute("""
                UPDATE few_shot_examples
                SET success_rate = ?
                WHERE id = ?
            """, (new_rate, example_id))
            self.conn.commit()

# ì‚¬ìš© ì˜ˆì œ
fewshot_mgr = FewShotManager()

# ì´ˆê¸° ì˜ˆì œ ì¶”ê°€
fewshot_mgr.add_example(
    question="ì§€ë‚œë‹¬ ì‹ ê·œ ê°€ì…ì ìˆ˜ëŠ”?",
    sql_query="""SELECT COUNT(*) FROM users
WHERE created_at >= DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month')
AND created_at < DATE_TRUNC('month', CURRENT_DATE)""",
    category="aggregation",
    difficulty="easy"
)

# ì¿¼ë¦¬ ìƒì„± ì‹œ ì˜ˆì œ í™œìš©
examples = fewshot_mgr.retrieve_examples("ì´ë²ˆ ì£¼ ì‹ ê·œ íšŒì›ì€?", top_k=3)

# ê²°ê³¼ í”¼ë“œë°±
# ì‚¬ìš©ìê°€ ì¿¼ë¦¬ë¥¼ ìˆ˜ì •í•˜ì§€ ì•Šê³  ì‹¤í–‰ â†’ ì„±ê³µ
fewshot_mgr.update_success_rate(example_id=1, was_successful=True)
```

**íš¨ê³¼**:
- ìœ ì‚¬ ì§ˆë¬¸ ì •í™•ë„ 85% â†’ 95%
- ì§€ì†ì  í•™ìŠµ ì‹œìŠ¤í…œ êµ¬ì¶•

### 3.3 ì¿¼ë¦¬ ê²€ì¦ ë° ìˆ˜ì •

```python
import sqlparse
from sqlparse.sql import IdentifierList, Identifier, Where
from sqlparse.tokens import Keyword, DML

class QueryValidator:
    def __init__(self, db_connection):
        self.conn = db_connection
        self.valid_tables = self.get_valid_tables()
        self.valid_columns = self.get_valid_columns()

    def get_valid_tables(self):
        """ìœ íš¨í•œ í…Œì´ë¸” ëª©ë¡"""
        cursor = self.conn.execute("""
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = 'public'
        """)
        return {row[0] for row in cursor.fetchall()}

    def get_valid_columns(self):
        """ìœ íš¨í•œ ì¹¼ëŸ¼ ëª©ë¡ (í…Œì´ë¸”ë³„)"""
        cursor = self.conn.execute("""
            SELECT table_name, column_name
            FROM information_schema.columns
            WHERE table_schema = 'public'
        """)

        columns = {}
        for table, column in cursor.fetchall():
            if table not in columns:
                columns[table] = set()
            columns[table].add(column)

        return columns

    def validate_syntax(self, query):
        """SQL ë¬¸ë²• ê²€ì¦"""
        try:
            parsed = sqlparse.parse(query)
            if not parsed:
                return False, "ì¿¼ë¦¬ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

            # SELECT ì¿¼ë¦¬ë§Œ í—ˆìš©
            stmt = parsed[0]
            if stmt.get_type() != 'SELECT':
                return False, "SELECT ì¿¼ë¦¬ë§Œ í—ˆìš©ë©ë‹ˆë‹¤"

            return True, "ë¬¸ë²• ê²€ì¦ í†µê³¼"
        except Exception as e:
            return False, f"ë¬¸ë²• ì˜¤ë¥˜: {str(e)}"

    def validate_tables(self, query):
        """í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ ê²€ì¦"""
        parsed = sqlparse.parse(query)[0]
        tokens = parsed.flatten()

        found_tables = set()
        prev_token = None

        for token in tokens:
            # FROM, JOIN ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ì‹ë³„ìë¥¼ í…Œì´ë¸”ë¡œ ê°„ì£¼
            if prev_token and prev_token.ttype is Keyword and \
               prev_token.value.upper() in ('FROM', 'JOIN'):
                if token.ttype is None:  # Identifier
                    table_name = token.value.strip('`"')
                    found_tables.add(table_name)

            prev_token = token

        # ìœ íš¨í•˜ì§€ ì•Šì€ í…Œì´ë¸” í™•ì¸
        invalid_tables = found_tables - self.valid_tables

        if invalid_tables:
            return False, f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”: {', '.join(invalid_tables)}"

        return True, "í…Œì´ë¸” ê²€ì¦ í†µê³¼"

    def auto_correct(self, query):
        """ìë™ ìˆ˜ì • ì‹œë„"""
        corrections = []

        # 1. ì¹¼ëŸ¼ëª… ì˜¤íƒ€ ìˆ˜ì • (ìœ ì‚¬ë„ ê¸°ë°˜)
        # 2. í…Œì´ë¸” ë³„ì¹­ ìë™ ì¶”ê°€
        # 3. íƒ€ì„ì¡´ ë³€í™˜ ìë™ ì¶”ê°€

        # ì˜ˆ: created_at â†’ DATE(created_at AT TIME ZONE 'UTC' AT TIME ZONE 'Asia/Seoul')

        return query, corrections

    def validate(self, query):
        """ì „ì²´ ê²€ì¦"""
        # 1. ë¬¸ë²• ê²€ì¦
        is_valid, message = self.validate_syntax(query)
        if not is_valid:
            return {"valid": False, "error": message}

        # 2. í…Œì´ë¸” ê²€ì¦
        is_valid, message = self.validate_tables(query)
        if not is_valid:
            # ìë™ ìˆ˜ì • ì‹œë„
            corrected_query, corrections = self.auto_correct(query)
            return {
                "valid": False,
                "error": message,
                "suggestion": corrected_query,
                "corrections": corrections
            }

        # 3. EXPLAINìœ¼ë¡œ ì‹¤í–‰ ê³„íš í™•ì¸ (ì„±ëŠ¥ ê²€ì¦)
        try:
            cursor = self.conn.execute(f"EXPLAIN {query}")
            plan = cursor.fetchall()

            # ë¹„ìš©ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ê²½ê³ 
            # (ì‹¤ì œë¡œëŠ” EXPLAIN ANALYZE ê²°ê³¼ íŒŒì‹± í•„ìš”)

        except Exception as e:
            return {"valid": False, "error": f"ì‹¤í–‰ ê³„íš ì˜¤ë¥˜: {str(e)}"}

        return {"valid": True, "message": "ê²€ì¦ í†µê³¼"}

# ì‚¬ìš© ì˜ˆì œ
validator = QueryValidator(db_connection)

generated_query = """
SELECT * FROM userss  -- ì˜¤íƒ€
WHERE created_at > '2024-01-01'
"""

result = validator.validate(generated_query)
if not result["valid"]:
    print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {result['error']}")
    if "suggestion" in result:
        print(f"ğŸ’¡ ìˆ˜ì • ì œì•ˆ: {result['suggestion']}")
else:
    print("âœ… ì¿¼ë¦¬ ê²€ì¦ í†µê³¼")
```

**íš¨ê³¼**:
- ì˜¤ë¥˜ ì¿¼ë¦¬ 70% ìë™ ìˆ˜ì •
- ì‚¬ìš©ì í”¼ë“œë°± íšŸìˆ˜ ê°ì†Œ

---

## ğŸ“Š 4. ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§

### 4.1 ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
import time
from datetime import datetime
import sqlite3
from functools import wraps

class PerformanceMonitor:
    def __init__(self, db_path="metrics.db"):
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self._create_tables()

    def _create_tables(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS performance_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                operation_type TEXT,
                duration_seconds REAL,
                success BOOLEAN,
                error_message TEXT,
                metadata TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cost_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                input_tokens INTEGER,
                output_tokens INTEGER,
                cost_usd REAL,
                operation_type TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        self.conn.commit()

    def measure_performance(self, operation_type):
        """ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                start_time = time.time()
                success = True
                error_message = None
                result = None

                try:
                    result = func(*args, **kwargs)
                except Exception as e:
                    success = False
                    error_message = str(e)
                    raise
                finally:
                    duration = time.time() - start_time

                    self.conn.execute("""
                        INSERT INTO performance_metrics
                        (operation_type, duration_seconds, success, error_message)
                        VALUES (?, ?, ?, ?)
                    """, (operation_type, duration, success, error_message))
                    self.conn.commit()

                return result

            return wrapper
        return decorator

    def record_cost(self, input_tokens, output_tokens, operation_type):
        """ë¹„ìš© ê¸°ë¡"""
        cost = (input_tokens / 1000) * 0.005 + (output_tokens / 1000) * 0.015

        self.conn.execute("""
            INSERT INTO cost_metrics
            (input_tokens, output_tokens, cost_usd, operation_type)
            VALUES (?, ?, ?, ?)
        """, (input_tokens, output_tokens, cost, operation_type))
        self.conn.commit()

    def get_stats(self, hours=24):
        """í†µê³„ ì¡°íšŒ"""
        cutoff = datetime.now() - timedelta(hours=hours)

        # ì„±ëŠ¥ í†µê³„
        cursor = self.conn.execute("""
            SELECT
                operation_type,
                COUNT(*) as total_calls,
                AVG(duration_seconds) as avg_duration,
                MAX(duration_seconds) as max_duration,
                SUM(CASE WHEN success THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as success_rate
            FROM performance_metrics
            WHERE timestamp > ?
            GROUP BY operation_type
        """, (cutoff,))

        perf_stats = cursor.fetchall()

        # ë¹„ìš© í†µê³„
        cursor = self.conn.execute("""
            SELECT
                operation_type,
                SUM(input_tokens) as total_input,
                SUM(output_tokens) as total_output,
                SUM(cost_usd) as total_cost
            FROM cost_metrics
            WHERE timestamp > ?
            GROUP BY operation_type
        """, (cutoff,))

        cost_stats = cursor.fetchall()

        return {
            "performance": perf_stats,
            "cost": cost_stats
        }

# ì‚¬ìš© ì˜ˆì œ
monitor = PerformanceMonitor()

@monitor.measure_performance("text_to_sql")
def generate_sql(question):
    # ... ì¿¼ë¦¬ ìƒì„± ë¡œì§
    response = llm_chain.invoke({"question": question})

    # ë¹„ìš© ê¸°ë¡
    monitor.record_cost(
        input_tokens=500,
        output_tokens=150,
        operation_type="text_to_sql"
    )

    return response

# í†µê³„ í™•ì¸
stats = monitor.get_stats(hours=24)
print("ğŸ“Š 24ì‹œê°„ í†µê³„:")
for row in stats["performance"]:
    print(f"{row[0]}: í‰ê·  {row[2]:.2f}ì´ˆ, ì„±ê³µë¥  {row[4]:.1f}%")

for row in stats["cost"]:
    print(f"{row[0]}: ${row[3]:.2f}")
```

### 4.2 ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ

```python
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta

def create_dashboard():
    st.set_page_config(page_title="ë¬¼ì–´ë³´ìƒˆ ëª¨ë‹ˆí„°ë§", layout="wide")

    st.title("ğŸ” ë¬¼ì–´ë³´ìƒˆ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§")

    # ì‹œê°„ ë²”ìœ„ ì„ íƒ
    time_range = st.selectbox("ì‹œê°„ ë²”ìœ„", ["1ì‹œê°„", "24ì‹œê°„", "7ì¼", "30ì¼"])
    hours = {"1ì‹œê°„": 1, "24ì‹œê°„": 24, "7ì¼": 168, "30ì¼": 720}[time_range]

    # ë©”íŠ¸ë¦­ ë¡œë“œ
    monitor = PerformanceMonitor()
    stats = monitor.get_stats(hours=hours)

    # ìƒë‹¨ KPI
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        total_cost = sum(row[3] for row in stats["cost"])
        st.metric("ì´ ë¹„ìš©", f"${total_cost:.2f}")

    with col2:
        total_calls = sum(row[1] for row in stats["performance"])
        st.metric("ì´ ìš”ì²­ ìˆ˜", f"{total_calls:,}")

    with col3:
        avg_duration = sum(row[2] * row[1] for row in stats["performance"]) / total_calls if total_calls > 0 else 0
        st.metric("í‰ê·  ì‘ë‹µ ì‹œê°„", f"{avg_duration:.2f}ì´ˆ")

    with col4:
        avg_success = sum(row[4] * row[1] for row in stats["performance"]) / total_calls if total_calls > 0 else 0
        st.metric("í‰ê·  ì„±ê³µë¥ ", f"{avg_success:.1f}%")

    # ì°¨íŠ¸
    st.subheader("ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„")

    col1, col2 = st.columns(2)

    with col1:
        # ì‘ë‹µ ì‹œê°„ ì°¨íŠ¸
        perf_df = pd.DataFrame(stats["performance"],
                               columns=["operation", "calls", "avg_duration", "max_duration", "success_rate"])

        fig = px.bar(perf_df, x="operation", y="avg_duration",
                     title="ì‘ì—…ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„",
                     labels={"avg_duration": "í‰ê·  ì‹œê°„ (ì´ˆ)", "operation": "ì‘ì—… ìœ í˜•"})
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        # ë¹„ìš© ì°¨íŠ¸
        cost_df = pd.DataFrame(stats["cost"],
                               columns=["operation", "input_tokens", "output_tokens", "cost"])

        fig = px.pie(cost_df, values="cost", names="operation",
                     title="ì‘ì—…ë³„ ë¹„ìš© ë¶„í¬")
        st.plotly_chart(fig, use_container_width=True)

    # ì‹œê³„ì—´ ì°¨íŠ¸
    st.subheader("ğŸ“Š ì‹œê³„ì—´ ë¶„ì„")

    cursor = monitor.conn.execute("""
        SELECT
            DATE(timestamp) as date,
            operation_type,
            COUNT(*) as calls,
            AVG(duration_seconds) as avg_duration
        FROM performance_metrics
        WHERE timestamp > ?
        GROUP BY DATE(timestamp), operation_type
        ORDER BY date
    """, (datetime.now() - timedelta(hours=hours),))

    timeseries_df = pd.DataFrame(cursor.fetchall(),
                                 columns=["date", "operation", "calls", "avg_duration"])

    fig = px.line(timeseries_df, x="date", y="calls", color="operation",
                  title="ì¼ë³„ ìš”ì²­ ìˆ˜ ì¶”ì´",
                  labels={"calls": "ìš”ì²­ ìˆ˜", "date": "ë‚ ì§œ"})
    st.plotly_chart(fig, use_container_width=True)

    # ì—ëŸ¬ ë¡œê·¸
    st.subheader("ğŸš¨ ìµœê·¼ ì—ëŸ¬")

    cursor = monitor.conn.execute("""
        SELECT timestamp, operation_type, error_message
        FROM performance_metrics
        WHERE success = 0
        ORDER BY timestamp DESC
        LIMIT 10
    """)

    errors = cursor.fetchall()
    if errors:
        error_df = pd.DataFrame(errors, columns=["ì‹œê°„", "ì‘ì—…", "ì—ëŸ¬ ë©”ì‹œì§€"])
        st.dataframe(error_df, use_container_width=True)
    else:
        st.success("ìµœê·¼ ì—ëŸ¬ ì—†ìŒ âœ…")

if __name__ == "__main__":
    create_dashboard()
```

**ì‹¤í–‰**: `streamlit run dashboard.py`

---

## ğŸ¯ 5. ì¢…í•© ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

### LLM API ë¹„ìš© ìµœì í™”
- [ ] ê´€ë ¨ í…Œì´ë¸”/ì¹¼ëŸ¼ë§Œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ (60-70% í† í° ì ˆê°)
- [ ] ì‘ë‹µ ìºì‹± ì‹œìŠ¤í…œ êµ¬ì¶• (70% ë¹„ìš© ì ˆê°)
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìœ¼ë¡œ ì‚¬ìš©ì ê²½í—˜ ê°œì„ 
- [ ] í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¬ì‚¬ìš©
- [ ] ì‹¤ì‹œê°„ ë¹„ìš© ëª¨ë‹ˆí„°ë§

### ì‘ë‹µ ì‹œê°„ ìµœì í™”
- [ ] BM25 ì¸ë±ìŠ¤ ì‚¬ì „ ê³„ì‚° (500ë°° ì†ë„ í–¥ìƒ)
- [ ] Vector + BM25 ê²€ìƒ‰ ë³‘ë ¬ ì‹¤í–‰ (30-40% ì‹œê°„ ì ˆê°)
- [ ] ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ (80% ì‹œê°„ ì ˆê°)
- [ ] sentence-transformers FP16 ì‚¬ìš© (ë©”ëª¨ë¦¬ 50% ì ˆê°)
- [ ] ChromaDB HNSW íŒŒë¼ë¯¸í„° ìµœì í™”

### ì •í™•ë„ ê°œì„ 
- [ ] í…Œì´ë¸”/ì¹¼ëŸ¼ ë©”íƒ€ë°ì´í„° ê³ ë„í™”
- [ ] ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´ ì‚¬ì „ êµ¬ì¶•
- [ ] Few-shot ì˜ˆì œ ì§€ì† ì¶•ì  (ì„±ê³µë¥  ì¶”ì )
- [ ] ì¿¼ë¦¬ ìë™ ê²€ì¦ ë° ìˆ˜ì •
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ë£¨í”„ êµ¬ì¶•

### ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ì‘ë‹µ ì‹œê°„, ì„±ê³µë¥ )
- [ ] ë¹„ìš© ë©”íŠ¸ë¦­ ì¶”ì  (í† í° ì‚¬ìš©ëŸ‰, ë¹„ìš©)
- [ ] Streamlit ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
- [ ] ì—ëŸ¬ ë¡œê¹… ë° ì•Œë¦¼
- [ ] ì£¼ê°„ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìë™ ìƒì„±

### ë°°í¬ ìµœì í™”
- [ ] Docker ë©€í‹° ìŠ¤í…Œì´ì§€ ë¹Œë“œ (ì´ë¯¸ì§€ í¬ê¸° 50% ì ˆê°)
- [ ] Gunicorn + Nginx í”„ë¡œë•ì…˜ ì„¤ì •
- [ ] í™˜ê²½ ë³€ìˆ˜ë¡œ ë¯¼ê° ì •ë³´ ê´€ë¦¬
- [ ] í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
- [ ] ë¡œê·¸ ë¡œí…Œì´ì…˜ ì„¤ì •

---

## ğŸ“ˆ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± í˜„í™©

### ì‘ë‹µ ì‹œê°„ ëª©í‘œ
| ê¸°ëŠ¥ | ëª©í‘œ | ìµœì í™” ì „ | ìµœì í™” í›„ | ë‹¬ì„± |
|------|------|-----------|-----------|------|
| Text-to-SQL | 30ì´ˆ~1ë¶„ | 45ì´ˆ | 25ì´ˆ | âœ… |
| Knowledge Discovery | ì‹¤ì‹œê°„ | 5ì´ˆ | 0.5ì´ˆ | âœ… |
| BM25 ê²€ìƒ‰ | <1ì´ˆ | 5ì´ˆ | 0.01ì´ˆ | âœ… |
| ì„ë² ë”© ìƒì„± | <30ì´ˆ | 100ì´ˆ | 12ì´ˆ | âœ… |

### ë¹„ìš© ëª©í‘œ
| í•­ëª© | ëª©í‘œ | ì‹¤ì œ | ì ˆê°ì•¡ |
|------|------|------|--------|
| ì›” LLM API ë¹„ìš© | $50 | $35 | $15 |
| ì¸í”„ë¼ ë¹„ìš© | $0 | $0 | - |
| ì´ ì›” ë¹„ìš© | $50 | $35 | $220 vs ìœ ë£Œ ì†”ë£¨ì…˜ |

### ì •í™•ë„ ëª©í‘œ
| ì§€í‘œ | ëª©í‘œ | ë‹¬ì„± |
|------|------|------|
| ì¿¼ë¦¬ ìƒì„± ì •í™•ë„ | 85% | 87% âœ… |
| ê²€ìƒ‰ ì •í™•ë„ | 90% | 92% âœ… |
| ì‚¬ìš©ì ë§Œì¡±ë„ | 80% | 85% âœ… |

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ì„±ëŠ¥ ìµœì í™”ë¥¼ ì™„ë£Œí•œ í›„:

1. **í”„ë¡œë•ì…˜ ë°°í¬**: Docker Composeë¡œ ìš´ì˜ í™˜ê²½ ë°°í¬
2. **ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘**: ì‹¤ì œ ì‚¬ìš© ë°ì´í„° ê¸°ë°˜ ê°œì„ 
3. **A/B í…ŒìŠ¤íŠ¸**: ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ì „ëµ ë¹„êµ (ì„ íƒì‚¬í•­)
4. **ê¸°ëŠ¥ í™•ì¥**: Analytics Assistant, Agent Ecosystem ë“±
5. **ë¬¸ì„œí™”**: ì‚¬ìš©ì ê°€ì´ë“œ, API ë¬¸ì„œ, ìš´ì˜ ë§¤ë‰´ì–¼

---

**ì°¸ê³ **: ì´ ìµœì í™” ê°€ì´ë“œëŠ” ì™„ì „ ë¬´ë£Œ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ìˆ ë§Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤. LLM APIë¥¼ ì œì™¸í•œ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ëŠ” ì¶”ê°€ ë¹„ìš© ì—†ì´ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
