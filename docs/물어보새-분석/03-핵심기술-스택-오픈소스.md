# í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ (ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜)

## ğŸ¯ ê¸°ìˆ  ì„ ì • ì›ì¹™

### ì˜¤í”ˆì†ŒìŠ¤ ìš°ì„  ì „ëµ
- **LLM APIë¥¼ ì œì™¸í•œ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ë¬´ë£Œ ì˜¤í”ˆì†ŒìŠ¤ ì‚¬ìš©**
- ìƒìš© SaaS ì„œë¹„ìŠ¤ ë°°ì œ (Pinecone, Weaviate Cloud, Langfuse ë“±)
- ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ ìš°ì„  ì„ íƒ
- ì»¤ë®¤ë‹ˆí‹° í™œì„±í™” ë° ì§€ì†ì  ìœ ì§€ë³´ìˆ˜ í™•ì¸

### ë¹„ìš© êµ¬ì¡°
```
ì´ ë¹„ìš© = LLM API ë¹„ìš©ë§Œ
- ì¸í”„ë¼ ë¹„ìš©: $0 (ì˜¤í”ˆì†ŒìŠ¤ + ë¡œì»¬ ì‹¤í–‰)
- DB ë¹„ìš©: $0 (PostgreSQL, SQLite)
- ë²¡í„° DB ë¹„ìš©: $0 (ChromaDB ë¡œì»¬)
- ì„ë² ë”© ë¹„ìš©: $0 (sentence-transformers ë¡œì»¬)
- ëª¨ë‹ˆí„°ë§ ë¹„ìš©: $0 (ìì²´ êµ¬ì¶•)
```

---

## ğŸ”§ í•µì‹¬ ê¸°ìˆ  ìƒì„¸

### 1. LangChain & LangGraph (í”„ë ˆì„ì›Œí¬)

#### LangChain
**ë¼ì´ì„ ìŠ¤**: MIT License (ì™„ì „ ë¬´ë£Œ)

**ì„ íƒ ì´ìœ **:
- LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ ì‚¬ì‹¤ìƒ í‘œì¤€ í”„ë ˆì„ì›Œí¬
- í’ë¶€í•œ ìƒíƒœê³„ (Document Loaders, Retrievers, Chains)
- í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹° ë° ì§€ì†ì  ì—…ë°ì´íŠ¸
- ì˜¤í”ˆì†ŒìŠ¤ ë¬´ë£Œ

**ì£¼ìš” ê¸°ëŠ¥**:
- **Chains**: Text-to-SQL, QA, Summarization ì²´ì¸
- **Document Loaders**: PDF, DOCX, Markdown, CSV ë“±
- **Text Splitters**: í† í° ê¸°ë°˜ ë¬¸ì„œ ë¶„í• 
- **Retrievers**: Vector Store í†µí•© ê²€ìƒ‰
- **Memory**: ëŒ€í™” ì´ë ¥ ê´€ë¦¬
- **Agents**: ReAct, Self-Ask ë“±

**ì„¤ì¹˜**:
```bash
pip install langchain==0.1.0
pip install langchain-community
pip install langchain-openai
```

**ì½”ë“œ ì˜ˆì‹œ**:
```python
from langchain.chains import create_sql_query_chain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0,
    openai_api_key="your-api-key"
)

# SQL ì²´ì¸ ìƒì„±
sql_chain = create_sql_query_chain(
    llm=llm,
    db=db,
    prompt=custom_prompt
)

# ì‹¤í–‰
result = sql_chain.invoke({"question": "ì‚¬ìš©ì ìˆ˜ëŠ”?"})
```

#### LangGraph
**ë¼ì´ì„ ìŠ¤**: ì˜¤í”ˆì†ŒìŠ¤ (ë¬´ë£Œ)

**ì„ íƒ ì´ìœ **:
- Multi-Agent ì›Œí¬í”Œë¡œìš° êµ¬ì¶•ì— ìµœì í™”
- State Machine ê¸°ë°˜ ë³µì¡í•œ ë¡œì§ ê´€ë¦¬
- Agent Supervisor íŒ¨í„´ ì§€ì›
- LangChainê³¼ ì™„ë²½í•œ í†µí•©

**ì£¼ìš” ê¸°ëŠ¥**:
- **State Graph**: ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°
- **Conditional Edges**: ì¡°ê±´ë¶€ ë¼ìš°íŒ…
- **Agent Supervisor**: Multi-Agent ì¡°ì •
- **Checkpointing**: ì›Œí¬í”Œë¡œìš° ì €ì¥/ë³µì›

**ì„¤ì¹˜**:
```bash
pip install langgraph==0.0.20
```

**ì½”ë“œ ì˜ˆì‹œ**:
```python
from langgraph.graph import StateGraph, END
from typing import TypedDict

# ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    question: str
    route: str
    result: str

# ê·¸ë˜í”„ ìƒì„±
workflow = StateGraph(AgentState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("router", route_question)
workflow.add_node("sql_agent", sql_agent)
workflow.add_node("knowledge_agent", knowledge_agent)

# ì¡°ê±´ë¶€ ì—£ì§€
workflow.add_conditional_edges(
    "router",
    lambda x: x["route"],
    {
        "sql": "sql_agent",
        "knowledge": "knowledge_agent"
    }
)

workflow.set_entry_point("router")
app = workflow.compile()
```

---

### 2. ChromaDB (Vector Database)

**ë¼ì´ì„ ìŠ¤**: Apache 2.0 (ì™„ì „ ë¬´ë£Œ)

**ì„ íƒ ì´ìœ **:
- **ì™„ì „ ë¡œì»¬ ì‹¤í–‰**: ì™¸ë¶€ ì„œë²„ ë¶ˆí•„ìš”
- **ì˜êµ¬ ì €ì¥ì†Œ**: DuckDB + Parquet ê¸°ë°˜
- **ë¹ ë¥¸ ì„±ëŠ¥**: ë°±ë§Œ ê°œ ë²¡í„°ë„ ë¹ ë¥¸ ê²€ìƒ‰
- **ë©”íƒ€ë°ì´í„° í•„í„°ë§**: ê°•ë ¥í•œ where ì ˆ ì§€ì›
- **ë¬´ë£Œ**: ìƒìš© Vector DB (Pinecone, Weaviate Cloud) ëŒ€ë¹„ $0

**ì£¼ìš” íŠ¹ì§•**:
- ì„ë² ë”© ìë™ ìƒì„± (ì„ íƒ ì‚¬í•­)
- ì½”ì‚¬ì¸ ìœ ì‚¬ë„, L2, IP ê±°ë¦¬ ë©”íŠ¸ë¦­
- ì»¬ë ‰ì…˜ ë‹¨ìœ„ ê´€ë¦¬
- Python, JavaScript í´ë¼ì´ì–¸íŠ¸

**ì„¤ì¹˜**:
```bash
pip install chromadb==0.4.22
```

**ê¸°ë³¸ ì‚¬ìš©**:
```python
import chromadb
from chromadb.config import Settings

# ì˜êµ¬ ì €ì¥ì†Œ í´ë¼ì´ì–¸íŠ¸
client = chromadb.Client(Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="./chromadb_data"
))

# ì»¬ë ‰ì…˜ ìƒì„±
collection = client.create_collection(
    name="my_collection",
    metadata={"description": "My first collection"}
)

# ë¬¸ì„œ ì¶”ê°€
collection.add(
    documents=["This is document 1", "This is document 2"],
    embeddings=[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]],
    metadatas=[{"source": "doc1"}, {"source": "doc2"}],
    ids=["id1", "id2"]
)

# ê²€ìƒ‰
results = collection.query(
    query_embeddings=[[0.1, 0.2, 0.3]],
    n_results=2,
    where={"source": "doc1"}  # ë©”íƒ€ë°ì´í„° í•„í„°
)

# ì˜êµ¬ ì €ì¥
client.persist()
```

**ê³ ê¸‰ ê¸°ëŠ¥**:
```python
# ê±°ë¦¬ ë©”íŠ¸ë¦­ ì„¤ì •
collection = client.create_collection(
    name="l2_collection",
    metadata={"hnsw:space": "l2"}  # cosine(ê¸°ë³¸), l2, ip
)

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„° + ë²¡í„°)
results = collection.query(
    query_embeddings=[[0.1, 0.2, 0.3]],
    n_results=10,
    where={
        "$and": [
            {"source": {"$eq": "wiki"}},
            {"timestamp": {"$gte": "2024-01-01"}}
        ]
    }
)

# ì—…ë°ì´íŠ¸
collection.update(
    ids=["id1"],
    documents=["Updated document"],
    metadatas=[{"source": "updated"}]
)

# ì‚­ì œ
collection.delete(ids=["id1"])
```

**ì„±ëŠ¥ íŠœë‹**:
```python
# HNSW ì¸ë±ìŠ¤ íŒŒë¼ë¯¸í„° ì¡°ì •
collection = client.create_collection(
    name="tuned_collection",
    metadata={
        "hnsw:space": "cosine",
        "hnsw:M": 16,  # ì—°ê²° ìˆ˜ (ê¸°ë³¸ 16, ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
        "hnsw:ef_construction": 200,  # êµ¬ì¶• ì‹œ íƒìƒ‰ ê¹Šì´
        "hnsw:ef": 10  # ê²€ìƒ‰ ì‹œ íƒìƒ‰ ê¹Šì´
    }
)
```

---

### 3. sentence-transformers (ì„ë² ë”© ëª¨ë¸)

**ë¼ì´ì„ ìŠ¤**: Apache 2.0 (ì™„ì „ ë¬´ë£Œ)

**ì„ íƒ ì´ìœ **:
- **ë¡œì»¬ ì‹¤í–‰**: OpenAI Embeddings API ëŒ€ë¹„ $0 ë¹„ìš©
- **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´ í¬í•¨ 100+ ì–¸ì–´
- **ë‹¤ì–‘í•œ ëª¨ë¸**: ê²½ëŸ‰ë¶€í„° ê³ ì„±ëŠ¥ê¹Œì§€
- **Hugging Face í†µí•©**: ìˆ˜ì²œ ê°œì˜ ì‚¬ì „í•™ìŠµ ëª¨ë¸

**ì¶”ì²œ ëª¨ë¸**:

| ëª¨ë¸ | í¬ê¸° | ì°¨ì› | ì„±ëŠ¥ | ì†ë„ | ìš©ë„ |
|------|------|------|------|------|------|
| `all-MiniLM-L6-v2` | 80MB | 384 | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | ê°œë°œ/í…ŒìŠ¤íŠ¸ |
| `all-mpnet-base-v2` | 420MB | 768 | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† | ê³ ì„±ëŠ¥ ì˜ì–´ |
| `paraphrase-multilingual-MiniLM-L12-v2` | 470MB | 384 | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† | ë‹¤êµ­ì–´ (ì¶”ì²œ) |
| `multilingual-e5-large` | 2.24GB | 1024 | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜†â˜†â˜† | ìµœê³  ì„±ëŠ¥ |

**ì„¤ì¹˜**:
```bash
pip install sentence-transformers==2.3.1
```

**ê¸°ë³¸ ì‚¬ìš©**:
```python
from sentence_transformers import SentenceTransformer

# ëª¨ë¸ ë¡œë“œ (ì²« ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ)
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
embedding = model.encode("ì•ˆë…•í•˜ì„¸ìš”")
print(embedding.shape)  # (384,)

# ë°°ì¹˜ ì„ë² ë”© (íš¨ìœ¨ì )
texts = ["ì²« ë²ˆì§¸ ë¬¸ì„œ", "ë‘ ë²ˆì§¸ ë¬¸ì„œ", "ì„¸ ë²ˆì§¸ ë¬¸ì„œ"]
embeddings = model.encode(texts, batch_size=32)
print(embeddings.shape)  # (3, 384)

# ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš© ì‹œ ì¶”ì²œ)
embeddings = model.encode(texts, normalize_embeddings=True)
```

**ìœ ì‚¬ë„ ê³„ì‚°**:
```python
from sentence_transformers import util
import torch

# ë¬¸ì„œ ì„ë² ë”©
doc_embeddings = model.encode(["ë¬¸ì„œ 1", "ë¬¸ì„œ 2", "ë¬¸ì„œ 3"])

# ì¿¼ë¦¬ ì„ë² ë”©
query_embedding = model.encode("ê²€ìƒ‰ ì¿¼ë¦¬")

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
similarities = util.cos_sim(query_embedding, doc_embeddings)
print(similarities)  # tensor([[0.8, 0.6, 0.3]])

# ìƒìœ„ Kê°œ ì„ íƒ
top_k = util.semantic_search(query_embedding, doc_embeddings, top_k=2)
print(top_k)  # [{'corpus_id': 0, 'score': 0.8}, {'corpus_id': 1, 'score': 0.6}]
```

**GPU ê°€ì†** (ì„ íƒ ì‚¬í•­):
```python
# GPU ì‚¬ìš© (CUDA ì„¤ì¹˜ í•„ìš”)
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device='cuda')

# CPU vs GPU ì†ë„ ë¹„êµ
# CPU: ~50 ë¬¸ì„œ/ì´ˆ
# GPU: ~500 ë¬¸ì„œ/ì´ˆ (10ë°° ë¹ ë¦„)
```

**ë©”ëª¨ë¦¬ ìµœì í™”**:
```python
# FP16 ì •ë°€ë„ (ë©”ëª¨ë¦¬ 50% ì ˆê°)
embeddings = model.encode(
    texts,
    convert_to_tensor=True,
    device='cuda',
    precision='float16'
)
```

---

### 4. rank-bm25 (BM25 ê²€ìƒ‰)

**ë¼ì´ì„ ìŠ¤**: Apache 2.0 (ì™„ì „ ë¬´ë£Œ)

**ì„ íƒ ì´ìœ **:
- **í‚¤ì›Œë“œ ê²€ìƒ‰ì˜ í™©ê¸ˆ í‘œì¤€**: TF-IDF ê°œì„  ì•Œê³ ë¦¬ì¦˜
- **ë¹ ë¥¸ ì†ë„**: ì‚¬ì „ ê³„ì‚°ìœ¼ë¡œ ìµœëŒ€ 500ë°° ê³ ì†í™”
- **ì™„ì „ ë¬´ë£Œ**: Elasticsearch ëŒ€ë¹„ $0
- **ê°„ë‹¨í•œ ì‚¬ìš©**: ë³„ë„ ì„œë²„ ë¶ˆí•„ìš”

**BM25 ì•Œê³ ë¦¬ì¦˜**:
```
BM25(Q, D) = Î£ IDF(qi) Ã— (f(qi, D) Ã— (k1 + 1)) / (f(qi, D) + k1 Ã— (1 - b + b Ã— |D| / avgdl))

- Q: ì¿¼ë¦¬
- D: ë¬¸ì„œ
- f(qi, D): ë¬¸ì„œ Dì—ì„œ qiì˜ ë¹ˆë„
- |D|: ë¬¸ì„œ Dì˜ ê¸¸ì´
- avgdl: í‰ê·  ë¬¸ì„œ ê¸¸ì´
- k1, b: íŠœë‹ íŒŒë¼ë¯¸í„°
```

**ì„¤ì¹˜**:
```bash
pip install rank-bm25==0.2.2
```

**ê¸°ë³¸ ì‚¬ìš©**:
```python
from rank_bm25 import BM25Okapi

# ì½”í¼ìŠ¤ (ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸)
corpus = [
    "ì‚¬ìš©ì í…Œì´ë¸”ì€ ì‚¬ìš©ì ì •ë³´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤",
    "ì£¼ë¬¸ í…Œì´ë¸”ì€ ì£¼ë¬¸ ë‚´ì—­ì„ ê´€ë¦¬í•©ë‹ˆë‹¤",
    "ìƒí’ˆ í…Œì´ë¸”ì€ ìƒí’ˆ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤"
]

# í† í°í™”
tokenized_corpus = [doc.split() for doc in corpus]

# BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
bm25 = BM25Okapi(tokenized_corpus)

# ê²€ìƒ‰
query = "ì‚¬ìš©ì ì •ë³´"
tokenized_query = query.split()

# ì ìˆ˜ ê³„ì‚°
scores = bm25.get_scores(tokenized_query)
print(scores)  # [2.15, 0.0, 0.0]

# ìƒìœ„ Kê°œ ë¬¸ì„œ
top_n = bm25.get_top_n(tokenized_query, corpus, n=2)
print(top_n)
# ['ì‚¬ìš©ì í…Œì´ë¸”ì€ ì‚¬ìš©ì ì •ë³´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤',
#  'ì£¼ë¬¸ í…Œì´ë¸”ì€ ì£¼ë¬¸ ë‚´ì—­ì„ ê´€ë¦¬í•©ë‹ˆë‹¤']
```

**í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Vector)**:
```python
import numpy as np
from sentence_transformers import SentenceTransformer

# ëª¨ë¸ ë¡œë“œ
embed_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

def hybrid_search(query, corpus, top_k=5, alpha=0.5):
    """
    alpha: BM25ì™€ ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜
    - alpha=1.0: BM25ë§Œ ì‚¬ìš©
    - alpha=0.0: ë²¡í„°ë§Œ ì‚¬ìš©
    - alpha=0.5: 50% BM25 + 50% ë²¡í„° (ì¶”ì²œ)
    """
    # BM25 ê²€ìƒ‰
    tokenized_query = query.split()
    bm25_scores = bm25.get_scores(tokenized_query)
    bm25_normalized = bm25_scores / (np.max(bm25_scores) + 1e-10)

    # ë²¡í„° ê²€ìƒ‰
    query_embedding = embed_model.encode(query)
    corpus_embeddings = embed_model.encode(corpus)
    vector_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    vector_normalized = vector_scores.numpy()

    # ì ìˆ˜ í†µí•©
    combined_scores = alpha * bm25_normalized + (1 - alpha) * vector_normalized

    # ìƒìœ„ Kê°œ
    top_indices = np.argsort(combined_scores)[-top_k:][::-1]
    return [(corpus[i], combined_scores[i]) for i in top_indices]

# ì‚¬ìš©
results = hybrid_search("ì‚¬ìš©ì í…Œì´ë¸” ì •ë³´", corpus, top_k=3, alpha=0.5)
for doc, score in results:
    print(f"{score:.3f}: {doc}")
```

**íŒŒë¼ë¯¸í„° íŠœë‹**:
```python
from rank_bm25 import BM25Plus, BM25L

# BM25Okapi (ê¸°ë³¸, ì¶”ì²œ)
bm25 = BM25Okapi(tokenized_corpus)

# BM25Plus (ì‘ì€ ë¬¸ì„œì— ìœ ë¦¬)
bm25_plus = BM25Plus(tokenized_corpus)

# BM25L (ê¸´ ë¬¸ì„œì— ìœ ë¦¬)
bm25_l = BM25L(tokenized_corpus)

# íŒŒë¼ë¯¸í„° ì¡°ì • (k1, b)
# k1: Term frequency saturation (ê¸°ë³¸ 1.5, ë²”ìœ„ 1.2-2.0)
# b: Length normalization (ê¸°ë³¸ 0.75, ë²”ìœ„ 0.0-1.0)
bm25 = BM25Okapi(tokenized_corpus, k1=1.2, b=0.75)
```

**ì‚¬ì „ ê³„ì‚° (ìµœëŒ€ 500ë°° ê³ ì†í™”)**:
```python
import pickle

# ì¸ë±ìŠ¤ êµ¬ì¶• ë° ì €ì¥ (ì˜¤í”„ë¼ì¸)
bm25 = BM25Okapi(tokenized_corpus)
with open('bm25_index.pkl', 'wb') as f:
    pickle.dump(bm25, f)

# ì¸ë±ìŠ¤ ë¡œë“œ (ì˜¨ë¼ì¸, ë§¤ìš° ë¹ ë¦„)
with open('bm25_index.pkl', 'rb') as f:
    bm25 = pickle.load(f)

# ê²€ìƒ‰ (0.1~1ms)
scores = bm25.get_scores(tokenized_query)
```

---

### 5. PostgreSQL (ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤)

**ë¼ì´ì„ ìŠ¤**: PostgreSQL License (ì™„ì „ ë¬´ë£Œ, ì˜¤í”ˆì†ŒìŠ¤)

**ì„ íƒ ì´ìœ **:
- **ê°€ì¥ ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ RDBMS**
- **í’ë¶€í•œ ê¸°ëŠ¥**: JSON, Full-text Search, Extensions
- **ì™„ì „ ë¬´ë£Œ**: ìƒìš© DB ëŒ€ë¹„ $0
- **í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°**: ì§€ì†ì  ì—…ë°ì´íŠ¸

**ì„¤ì¹˜** (macOS):
```bash
brew install postgresql@15
brew services start postgresql@15
```

**ì„¤ì¹˜** (Ubuntu):
```bash
sudo apt update
sudo apt install postgresql postgresql-contrib
sudo systemctl start postgresql
```

**Python ì—°ê²°**:
```bash
pip install psycopg2-binary sqlalchemy
```

**SQLAlchemy ì‚¬ìš©**:
```python
from sqlalchemy import create_engine, text

# ì—°ê²°
engine = create_engine("postgresql://user:password@localhost:5432/mydb")

# ì¿¼ë¦¬ ì‹¤í–‰
with engine.connect() as conn:
    result = conn.execute(text("SELECT * FROM users LIMIT 10"))
    for row in result:
        print(row)

# LangChain í†µí•©
from langchain.sql_database import SQLDatabase

db = SQLDatabase(engine)
table_info = db.get_table_info(["users", "orders"])
```

**ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘**:
```python
from sqlalchemy import inspect

inspector = inspect(engine)

# ëª¨ë“  í…Œì´ë¸” ì¡°íšŒ
tables = inspector.get_table_names()

# í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
for table in tables:
    columns = inspector.get_columns(table)
    for col in columns:
        print(f"{table}.{col['name']}: {col['type']}")
```

---

### 6. SQLite (ëŒ€í™” ì´ë ¥, ìºì‹œ)

**ë¼ì´ì„ ìŠ¤**: Public Domain (ì™„ì „ ë¬´ë£Œ)

**ì„ íƒ ì´ìœ **:
- **ë³„ë„ ì„œë²„ ë¶ˆí•„ìš”**: íŒŒì¼ ê¸°ë°˜ DB
- **ì´ˆê²½ëŸ‰**: ëª‡ë°± KB
- **ë¹ ë¥¸ ì„±ëŠ¥**: ë¡œì»¬ íŒŒì¼ I/O
- **ì™„ì „ ë¬´ë£Œ**: ë¼ì´ì„ ìŠ¤ ì œì•½ ì—†ìŒ

**ì„¤ì¹˜**: Python í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í¬í•¨ (ë³„ë„ ì„¤ì¹˜ ë¶ˆí•„ìš”)

**ê¸°ë³¸ ì‚¬ìš©**:
```python
import sqlite3

# DB ì—°ê²° (íŒŒì¼ ìƒì„±)
conn = sqlite3.connect('conversations.db')
cursor = conn.cursor()

# í…Œì´ë¸” ìƒì„±
cursor.execute('''
CREATE TABLE IF NOT EXISTS conversations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    role TEXT,
    content TEXT
)
''')

# ì‚½ì…
cursor.execute('''
INSERT INTO conversations (session_id, role, content)
VALUES (?, ?, ?)
''', ('abc123', 'user', 'ì‚¬ìš©ì ìˆ˜ëŠ”?'))

conn.commit()

# ì¡°íšŒ
cursor.execute('SELECT * FROM conversations WHERE session_id = ?', ('abc123',))
rows = cursor.fetchall()

# ì¢…ë£Œ
conn.close()
```

**Context Manager ì‚¬ìš©**:
```python
with sqlite3.connect('conversations.db') as conn:
    cursor = conn.cursor()
    cursor.execute('SELECT * FROM conversations')
    rows = cursor.fetchall()
```

---

### 7. Streamlit (ì›¹ UI)

**ë¼ì´ì„ ìŠ¤**: Apache 2.0 (ì™„ì „ ë¬´ë£Œ)

**ì„ íƒ ì´ìœ **:
- **ë¹ ë¥¸ ê°œë°œ**: Pythonë§Œìœ¼ë¡œ ì›¹ ì•± êµ¬ì¶•
- **ëŒ€í™”í˜• UI**: ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ë‚´ì¥
- **ë¬´ë£Œ ë°°í¬**: Streamlit Cloud ë¬´ë£Œ í”Œëœ
- **í’ë¶€í•œ ì»´í¬ë„ŒíŠ¸**: ì°¨íŠ¸, í¼, ë ˆì´ì•„ì›ƒ

**ì„¤ì¹˜**:
```bash
pip install streamlit==1.30.0
```

**ê¸°ë³¸ ì±„íŒ… ì•±**:
```python
import streamlit as st

st.title("AI ë°ì´í„° ë¶„ì„ê°€")

# ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # AI ì‘ë‹µ
    with st.chat_message("assistant"):
        response = generate_response(prompt)  # AI ë¡œì§
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})
```

**ì‹¤í–‰**:
```bash
streamlit run app.py
```

---

### 8. ë¬¸ì„œ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

#### PyPDF2 (PDF íŒŒì‹±)
```bash
pip install PyPDF2
```

```python
import PyPDF2

with open('document.pdf', 'rb') as file:
    pdf_reader = PyPDF2.PdfReader(file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()
```

#### python-docx (Word ë¬¸ì„œ)
```bash
pip install python-docx
```

```python
from docx import Document

doc = Document('document.docx')
text = "\n".join([para.text for para in doc.paragraphs])
```

#### markdown (Markdown íŒŒì‹±)
```bash
pip install markdown
```

```python
import markdown

with open('README.md', 'r') as f:
    md_text = f.read()
    html = markdown.markdown(md_text)
```

---

## ğŸ“Š ê¸°ìˆ  ìŠ¤íƒ ë¹„êµ

### ìƒìš© SaaS vs ì˜¤í”ˆì†ŒìŠ¤

| ê¸°ëŠ¥ | ìƒìš© SaaS | ì˜¤í”ˆì†ŒìŠ¤ (ìš°ë¦¬ ì„ íƒ) | ë¹„ìš© ì ˆê° |
|------|-----------|---------------------|----------|
| Vector DB | Pinecone (~$70/ì›”) | ChromaDB (ë¡œì»¬) | $70/ì›” |
| ì„ë² ë”© | OpenAI Embeddings (~$20/ì›”) | sentence-transformers | $20/ì›” |
| ê²€ìƒ‰ | Elasticsearch (~$50/ì›”) | BM25 (Python) | $50/ì›” |
| ëª¨ë‹ˆí„°ë§ | Langfuse (~$30/ì›”) | SQLite ë¡œê¹… | $30/ì›” |
| DB | AWS RDS (~$50/ì›”) | PostgreSQL ë¡œì»¬ | $50/ì›” |
| **ì´ ì ˆê°** | | | **$220/ì›”** |

**ì—°ê°„ ì ˆê°**: $2,640 (316ë§Œì›)

---

## ğŸš€ ì „ì²´ ì„¤ì¹˜ ê°€ì´ë“œ

### requirements.txt
```txt
# LLM & Framework
langchain==0.1.0
langchain-community==0.0.13
langchain-openai==0.0.5
langgraph==0.0.20
openai==1.10.0

# Vector DB & Search
chromadb==0.4.22
sentence-transformers==2.3.1
rank-bm25==0.2.2

# Database
psycopg2-binary==2.9.9
sqlalchemy==2.0.25

# Document Processing
PyPDF2==3.0.1
python-docx==1.1.0
markdown==3.5.2

# Web UI
streamlit==1.30.0

# Utilities
python-dotenv==1.0.0
requests==2.31.0
```

### ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash

# Python 3.10+ í™•ì¸
python --version

# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# PostgreSQL ì„¤ì¹˜ (macOS)
brew install postgresql@15
brew services start postgresql@15

# DB ìƒì„±
createdb ai_data_tool

echo "ì„¤ì¹˜ ì™„ë£Œ!"
```

---

ì´ì œ ëª¨ë“  ì˜¤í”ˆì†ŒìŠ¤ ê¸°ìˆ  ìŠ¤íƒì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ ë¬¸ì„œì—ì„œ ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œë¥¼ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤.
